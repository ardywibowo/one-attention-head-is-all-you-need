{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKyC3mZ72qlH"
      },
      "source": [
        "Copy-paste the contents of this notebook at the beginning of every other notebook in this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBe5K-JpHXFs"
      },
      "source": [
        "## Variable hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "httbkgCwHYF4"
      },
      "outputs": [],
      "source": [
        "# Fixed length of list to be sorted\n",
        "MAX_LIST_LENGTH = 50\n",
        "\n",
        "# Size of vocabulary\n",
        "NUM_SORT_TOKENS = 4\n",
        "D_VOCAB = NUM_SORT_TOKENS + 3\n",
        "\n",
        "# Should lists have repetitions?\n",
        "ALLOW_REPETITIONS = True\n",
        "\n",
        "# Attention only? (False -> model includes MLPs)\n",
        "ATTN_ONLY = True\n",
        "# ATTN_ONLY = False\n",
        "\n",
        "# Model dimensions\n",
        "N_LAYERS = 1\n",
        "N_HEADS = 1\n",
        "D_MODEL = 128\n",
        "D_HEAD = 32\n",
        "D_MLP = None\n",
        "\n",
        "if ATTN_ONLY:\n",
        "    D_MLP = None\n",
        "\n",
        "# Default batch size\n",
        "DEFAULT_BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o--LiD_HMGs"
      },
      "source": [
        "## Prelude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yjVSLZbEEJX"
      },
      "source": [
        "### Install and import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wqa5uVY-2oC7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/randyardywibowo/miniconda3/envs/lit/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import transformer_lens\n",
        "except:\n",
        "    !pip install git+https://github.com/neelnanda-io/TransformerLens\n",
        "    !pip install circuitsvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "dqFfW5V32yaO",
        "outputId": "323df961-4b90-49e8-8ab8-ef66c85061d1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div id=\"circuits-vis-eecd94fd-41c8\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, Hello } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-eecd94fd-41c8\",\n",
              "      Hello,\n",
              "      {\"name\": \"You\"}\n",
              "    )\n",
              "    </script>"
            ],
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x29187f700>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime as dt\n",
        "from itertools import repeat\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "from typing import cast, Generator, Literal\n",
        "\n",
        "import circuitsvis as cv\n",
        "from fancy_einsum import einsum\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, tensor, Tensor, TensorType as TT\n",
        "from torch.nn import functional as F\n",
        "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
        "from tqdm import tqdm\n",
        "from typing_extensions import Self\n",
        "\n",
        "cv.examples.hello(\"You\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo4oP4dSrYhD"
      },
      "source": [
        "### Invariable hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XwivtyM_Hru",
        "outputId": "253931dc-5c4d-4f11-a4e9-b80730e9372c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE = 'mps'\n"
          ]
        }
      ],
      "source": [
        "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE = \"mps\"\n",
        "print(f\"{DEVICE = }\")\n",
        "\n",
        "# Seeds to generate training, validation, and test data\n",
        "TRAIN_SEED = 42\n",
        "VAL_SEED = 66\n",
        "TEST_SEED = 1729\n",
        "\n",
        "# Context length: [start, *(unsorted_)list_length, mid, *(sorted_)list_length]\n",
        "N_CTX = 2 * MAX_LIST_LENGTH + 2\n",
        "\n",
        "# \"Real\" tokens range from 0 to D_VOCAB - 2 (non-inclusive)\n",
        "VOCAB_MIN_ID = 0\n",
        "VOCAB_MAX_ID = NUM_SORT_TOKENS\n",
        "\n",
        "# START token is D_VOCAB - 2 and MID token is D_VOCAB - 1\n",
        "END_TOKEN_ID = D_VOCAB - 3\n",
        "START_TOKEN_ID = D_VOCAB - 2\n",
        "MID_TOKEN_ID = D_VOCAB - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1, 2, 3]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_list = list(range(VOCAB_MIN_ID, VOCAB_MAX_ID))\n",
        "vocab_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlWGwox63j24"
      },
      "source": [
        "### Data generator and datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44OZjOq83k1H",
        "outputId": "413c82ad-aa84-42bc-f9ab-fe4c0608481a"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from itertools import combinations\n",
        "\n",
        "vocab_list = list(range(VOCAB_MIN_ID, VOCAB_MAX_ID))\n",
        "\n",
        "def map_integers_to_subsets(lst):\n",
        "    subset_map = {}\n",
        "    # Create subsets for all lengths\n",
        "    for i in range(len(lst) + 1):\n",
        "        for subset in combinations(lst, i):\n",
        "            # Convert subset to list (because combinations returns tuples)\n",
        "            subset_map[len(subset_map)] = list(subset)\n",
        "    return subset_map\n",
        "\n",
        "subset_map = map_integers_to_subsets(vocab_list)\n",
        "subset_map.pop(0)\n",
        "\n",
        "def generate_unique_list(total_size: int) -> torch.Tensor:\n",
        "    # Generate a list of unique sequences\n",
        "    start_time = time.time()\n",
        "    unique_sequences = set()\n",
        "    while len(unique_sequences) < total_size:\n",
        "        if ALLOW_REPETITIONS:\n",
        "            # Generate random number between 0 and 2\n",
        "            random_number = random.choices(list(subset_map.keys()), k=1)[0]\n",
        "            random_subset = subset_map[random_number]\n",
        "            random_k = random.randint(1, MAX_LIST_LENGTH)\n",
        "            \n",
        "            sequence = tuple(random.choices(random_subset, k=random_k))\n",
        "        \n",
        "        sequence = (START_TOKEN_ID,) + sequence + (MID_TOKEN_ID,) + tuple(sorted(sequence))\n",
        "        \n",
        "        # Pad with end tokens until the sequence is of length MAX_LIST_LENGTH\n",
        "        MAX_POSSIBLE_LENGTH = 2 * MAX_LIST_LENGTH + 2\n",
        "        sequence = sequence + (END_TOKEN_ID,) * (MAX_POSSIBLE_LENGTH - len(sequence))\n",
        "        \n",
        "        unique_sequences.add(sequence)\n",
        "        \n",
        "        # Stop after 60 seconds\n",
        "        if time.time() - start_time > 60:\n",
        "            break  # Exit the loop\n",
        "\n",
        "    # Convert to tensor and move to the correct DEVICE\n",
        "    return torch.tensor(list(unique_sequences), dtype=torch.int32)\n",
        "\n",
        "def process_data(data: torch.Tensor) -> torch.Tensor:\n",
        "    # Sort the data\n",
        "    data_sorted = torch.sort(data, dim=1).values\n",
        "    # Add START tokens\n",
        "    data_start = START_TOKEN_ID * torch.ones(data.size(0), 1, dtype=torch.int32)\n",
        "    # Add MID tokens\n",
        "    data_mid = MID_TOKEN_ID * torch.ones(data.size(0), 1, dtype=torch.int32)\n",
        "    # Concatenate all parts\n",
        "    return torch.cat((data_start, data, data_mid, data_sorted), dim=1)\n",
        "\n",
        "# Set the seeds\n",
        "torch.manual_seed(TRAIN_SEED)\n",
        "data = generate_unique_list(2_000_000 + 200_000)  # extra for validation and test\n",
        "# data = generate_unique_list(100_000)  # extra for validation and test\n",
        "\n",
        "# if not ALLOW_REPETITIONS:\n",
        "#     # Generate the complete list of unique sequences\n",
        "#     data = generate_unique_list(1_000_000 + 200_000)  # extra for validation and test\n",
        "# else:\n",
        "#     # If repetitions are allowed, just generate a large random tensor\n",
        "#     data = torch.randint(VOCAB_MIN_ID, VOCAB_MAX_ID, (1_200_000, LIST_LENGTH), dtype=torch.int32)\n",
        "\n",
        "# Process the data with start, mid tokens and sorted sequences\n",
        "# full_data = process_data(data)\n",
        "full_data = data\n",
        "full_data = full_data.long()\n",
        "\n",
        "num_samples = full_data.size(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([5, 3, 1, 1, 3, 1, 3, 3, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 1, 1, 1, 1, 1, 3,\n",
              "        3, 3, 3, 1, 1, 3, 3, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "        4, 4, 4, 4, 4, 4])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_data[0]\n",
        "# END_TOKEN_ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the length of splits for train, validation, and test sets\n",
        "train_length = num_samples * 8 // 10\n",
        "val_length = num_samples * 1 // 10\n",
        "test_length = num_samples - train_length - val_length\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "train_data, val_data, test_data = random_split(\n",
        "    full_data, [train_length, val_length, test_length],\n",
        "    generator=torch.Generator().manual_seed(42)  # for reproducibility\n",
        ")\n",
        "\n",
        "# Create DataLoaders for each set\n",
        "# Wrap the actual data in a TensorDataset\n",
        "train_loader = DataLoader(TensorDataset(train_data.dataset[train_data.indices]), batch_size=DEFAULT_BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(val_data.dataset[val_data.indices]), batch_size=1024, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(test_data.dataset[test_data.indices]), batch_size=1024, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2200000"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MID_TOKEN_ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([5, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3,\n",
              "        3, 3, 1, 1, 3, 1, 3, 1, 3, 3, 1, 1, 3, 1, 1, 1, 3, 3, 3, 1, 3, 6, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4,\n",
              "        4, 4, 4, 4, 4, 4])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(test_loader))[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBJn1fKm3pMX"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTlNfaKY3qkd",
        "outputId": "7d962ac8-efe5-4681-bb20-a4ff8b675156"
      },
      "outputs": [],
      "source": [
        "def loss_fn(\n",
        "    logits: Tensor, # [batch, pos, d_vocab] \n",
        "    tokens: Tensor, # [batch, pos] \n",
        "    return_per_token: bool = False\n",
        ") -> Tensor: # scalar\n",
        "    \"\"\"Mean cross-entropy between tokens in the sorted list part of the \n",
        "    sequence (last `LIST_LENGTH`) and model's predictions about them.\n",
        "    \"\"\"\n",
        "    sorted_start_pos = MAX_LIST_LENGTH + 2\n",
        "    logits = logits[:, (sorted_start_pos-1):-1]\n",
        "    tokens = tokens[:, sorted_start_pos : None]\n",
        "    log_probs = logits.log_softmax(-1)\n",
        "    correct_log_probs = log_probs.gather(-1, tokens[..., None])[..., 0]\n",
        "    if return_per_token:\n",
        "        return -correct_log_probs\n",
        "    return -correct_log_probs.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKKrzLEq3sun"
      },
      "source": [
        "### Accuracy and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1x6q7b7O3rZa"
      },
      "outputs": [],
      "source": [
        "def truncate_sequence(base_sequence, sequence_to_truncate, mid_token_id, end_token_id):\n",
        "    try:\n",
        "        # Finding the truncation points in the base sequence\n",
        "        mid_index = base_sequence.index(mid_token_id) + 1\n",
        "        end_index = base_sequence.index(end_token_id)\n",
        "\n",
        "        # Truncating the second sequence using the same indices\n",
        "        truncated_sequence = sequence_to_truncate[mid_index:end_index]\n",
        "        return truncated_sequence\n",
        "    except ValueError:\n",
        "        # Handle the case where MID_TOKEN_ID or END_TOKEN_ID is not found\n",
        "        return []\n",
        "\n",
        "def get_diff_row_inds(\n",
        "    a: Tensor, # [dim1, dim2]\n",
        "    b: Tensor  # [dim1, dim2]\n",
        ") -> Tensor:   # [dim1]\n",
        "    \"\"\"Find indices of rows where a and b differ\"\"\"\n",
        "    assert a.shape == b.shape\n",
        "    return ((a == b).prod(dim=1) == 0).nonzero(as_tuple=True)[0]\n",
        "\n",
        "def acc_fn(\n",
        "    logits: Tensor, # [batch, pos, d_vocab]\n",
        "    tokens: Tensor, # [batch, pos]\n",
        "    per: Literal[\"token\", \"sequence\"] = \"sequence\"\n",
        ") -> float:\n",
        "    \"\"\"Compute accuracy as percentage of correct predictions\"\"\"\n",
        "    assert per in (\"token\", \"sequence\")\n",
        "    # Get logits of predictions for position\n",
        "    preds = logits.argmax(-1).to(tokens.device)\n",
        "    preds = preds[:, :-1]\n",
        "    tokens = tokens[:, 1:]\n",
        "    tokens = tokens.detach().cpu().numpy().tolist()\n",
        "    preds = preds.detach().cpu().numpy().tolist()\n",
        "    \n",
        "    new_preds = []\n",
        "    new_tokens = []\n",
        "    for pred, token in zip(preds, tokens):\n",
        "        new_preds.append(truncate_sequence(token, pred, MID_TOKEN_ID, END_TOKEN_ID))\n",
        "        new_tokens.append(truncate_sequence(token, token, MID_TOKEN_ID, END_TOKEN_ID))\n",
        "    \n",
        "    preds = new_preds\n",
        "    tokens = new_tokens\n",
        "    \n",
        "    # print(\"Preds: \", preds)\n",
        "    # print(\"Tokens: \", tokens)\n",
        "    \n",
        "    all_correct = [p == t for p, t in zip(preds, tokens)]\n",
        "    acc = sum(all_correct) / len(all_correct)\n",
        "    \n",
        "    return acc\n",
        "\n",
        "def validate(\n",
        "    model: HookedTransformer, \n",
        "    data: Tensor, # [batch, pos]\n",
        "    per: Literal[\"token\", \"sequence\"] = \"sequence\"\n",
        ") -> float:\n",
        "    \"\"\"Test this model on `data`\"\"\"\n",
        "    logits = model(data)\n",
        "    acc = acc_fn(logits, tokens=data, per=per)\n",
        "    return acc\n",
        "\n",
        "def show_mispreds(\n",
        "    model: HookedTransformer, \n",
        "    data: Tensor # [batch, pos]\n",
        ") -> None:\n",
        "    \"\"\"Test this model on `data` and print mispredictions\"\"\"\n",
        "    logits = model(data)\n",
        "    sorted_start_pos = MAX_LIST_LENGTH + 2\n",
        "    logits = logits[:, (sorted_start_pos-1):-1]\n",
        "    tokens = data[:, sorted_start_pos:].cpu()\n",
        "    preds = logits.argmax(-1).cpu()\n",
        "    mispred_inds = get_diff_row_inds(tokens, preds)\n",
        "    for i in mispred_inds:\n",
        "        print(f\"[{i}] {tokens[i].numpy().tolist()} | {preds[i].numpy().tolist()}\")\n",
        "    print(f\"{len(mispred_inds)}/{len(preds)} ({len(mispred_inds) / len(preds) :.2%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5trW5nKhF3I"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Hj7N1nhHrI"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YwIffvImhT4g"
      },
      "outputs": [],
      "source": [
        "cfg = HookedTransformerConfig(\n",
        "    d_model=D_MODEL,\n",
        "    n_layers=N_LAYERS,\n",
        "    n_heads=N_HEADS,\n",
        "    d_head=D_HEAD,\n",
        "    n_ctx=N_CTX,\n",
        "    d_vocab=D_VOCAB,\n",
        "    act_fn=\"relu\",\n",
        "    seed=42,\n",
        "    device=DEVICE,\n",
        "    attn_only=ATTN_ONLY\n",
        ")\n",
        "model = HookedTransformer(cfg, move_to_device=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model.pos_embed.W_pos.data[:] = 0\n",
        "# model.pos_embed.W_pos.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vYV7QAd3vCT"
      },
      "source": [
        "### Training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LHBPL3Xw3uAt"
      },
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class TrainingHistory:\n",
        "    losses: list[float]\n",
        "    train_accuracies: list[float]\n",
        "    val_accuracies: list[float]\n",
        "\n",
        "def converged(val_accs: list[float], n_last: int = 10) -> bool:\n",
        "    return cast(bool, (tensor(val_accs[-n_last:]) == 1).all().item())\n",
        "\n",
        "# Number of epochs\n",
        "n_epochs = 20000\n",
        "# n_epochs = 2500\n",
        "\n",
        "# Optimization\n",
        "lr = 1e-3\n",
        "betas = (.9, .999)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \"min\", patience=200)\n",
        "\n",
        "# Training data generator\n",
        "def train_model(model: HookedTransformer) -> TrainingHistory:\n",
        "    losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "    \n",
        "    train_gen = iter(train_loader)\n",
        "    for epoch in range(n_epochs):\n",
        "        \n",
        "        try:\n",
        "            tokens = next(train_gen)[0].to(device=DEVICE)\n",
        "        except StopIteration:\n",
        "            train_gen = iter(train_loader)\n",
        "            tokens = next(train_gen)[0].to(device=DEVICE)\n",
        "        \n",
        "        logits = model(tokens)\n",
        "        loss = loss_fn(logits, tokens)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        scheduler.step(loss)\n",
        "        \n",
        "        if epoch % 100 == 0:\n",
        "            losses.append(loss.item())\n",
        "            train_batch_acc = acc_fn(logits, tokens)\n",
        "            \n",
        "            val_data = next(iter(val_loader))[0].to(device=DEVICE)\n",
        "            val_acc = validate(model, val_data)\n",
        "            val_loss = loss_fn(model(val_data), val_data)\n",
        "\n",
        "            train_accuracies.append(train_batch_acc)\n",
        "            val_accuracies.append(val_acc)\n",
        "            print(\n",
        "                f\"Epoch {epoch}/{n_epochs} ({epoch / n_epochs:.0%}) : \"\n",
        "                f\"loss = {loss.item():.4f}; {train_batch_acc=:.3%}; \"\n",
        "                f\"{val_acc=:.3%}; lr={scheduler._last_lr[0]}\" #type:ignore\n",
        "            )\n",
        "            # If last 10 recorded val_accuracies are 100%\n",
        "            if converged(val_accuracies):\n",
        "                print(f\"\\nAchieved consistent perfect validation accuracy after {epoch} epochs\")\n",
        "                break\n",
        "    return TrainingHistory(losses, train_accuracies, val_accuracies)\n",
        "\n",
        "def load_model_state(model: HookedTransformer, filename: str) -> None:\n",
        "    assert os.path.isdir(\"models\"), \"Make a directory `models` with model state dicts\"\n",
        "    if not filename.startswith(\"models/\"):\n",
        "        filename = f\"models/{filename}\"\n",
        "    with open(filename, \"rb\") as f:\n",
        "        state_dict = pickle.load(f)\n",
        "    model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([128, 102])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_gen = iter(train_loader)\n",
        "next(train_gen)[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXhR2JBjdXkD"
      },
      "source": [
        "### Train or load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CMH8DwNHdWLA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/20000 (0%) : loss = 2.4968; train_batch_acc=5.469%; val_acc=1.953%; lr=0.001\n",
            "Epoch 100/20000 (0%) : loss = 0.1009; train_batch_acc=1.562%; val_acc=2.441%; lr=0.001\n",
            "Epoch 200/20000 (1%) : loss = 0.0688; train_batch_acc=0.000%; val_acc=2.051%; lr=0.001\n",
            "Epoch 300/20000 (2%) : loss = 0.0788; train_batch_acc=5.469%; val_acc=2.051%; lr=0.001\n",
            "Epoch 400/20000 (2%) : loss = 0.0655; train_batch_acc=1.562%; val_acc=2.344%; lr=0.001\n",
            "Epoch 500/20000 (2%) : loss = 0.0605; train_batch_acc=2.344%; val_acc=1.855%; lr=0.001\n",
            "Epoch 600/20000 (3%) : loss = 0.0541; train_batch_acc=2.344%; val_acc=3.516%; lr=0.0001\n",
            "Epoch 700/20000 (4%) : loss = 0.0587; train_batch_acc=3.125%; val_acc=2.930%; lr=0.0001\n",
            "Epoch 800/20000 (4%) : loss = 0.0576; train_batch_acc=1.562%; val_acc=2.930%; lr=0.0001\n",
            "Epoch 900/20000 (4%) : loss = 0.0629; train_batch_acc=3.125%; val_acc=2.832%; lr=0.0001\n",
            "Epoch 1000/20000 (5%) : loss = 0.0632; train_batch_acc=1.562%; val_acc=1.953%; lr=0.0001\n",
            "Epoch 1100/20000 (6%) : loss = 0.0544; train_batch_acc=4.688%; val_acc=2.344%; lr=0.0001\n",
            "Epoch 1200/20000 (6%) : loss = 0.0461; train_batch_acc=3.125%; val_acc=3.418%; lr=1e-05\n",
            "Epoch 1300/20000 (6%) : loss = 0.0585; train_batch_acc=1.562%; val_acc=3.027%; lr=1e-05\n",
            "Epoch 1400/20000 (7%) : loss = 0.0594; train_batch_acc=0.781%; val_acc=2.051%; lr=1.0000000000000002e-06\n",
            "Epoch 1500/20000 (8%) : loss = 0.0488; train_batch_acc=0.781%; val_acc=3.516%; lr=1.0000000000000002e-06\n",
            "Epoch 1600/20000 (8%) : loss = 0.0596; train_batch_acc=1.562%; val_acc=2.637%; lr=1.0000000000000002e-06\n",
            "Epoch 1700/20000 (8%) : loss = 0.0544; train_batch_acc=5.469%; val_acc=3.223%; lr=1.0000000000000002e-07\n",
            "Epoch 1800/20000 (9%) : loss = 0.0531; train_batch_acc=2.344%; val_acc=3.418%; lr=1.0000000000000002e-07\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/randyardywibowo/Github/one-attention-head-is-all-you-need/notebooks/03_VariableLength.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/randyardywibowo/Github/one-attention-head-is-all-you-need/notebooks/03_VariableLength.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m train_model(model)\n",
            "\u001b[1;32m/Users/randyardywibowo/Github/one-attention-head-is-all-you-need/notebooks/03_VariableLength.ipynb Cell 30\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/randyardywibowo/Github/one-attention-head-is-all-you-need/notebooks/03_VariableLength.ipynb#X36sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits, tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/randyardywibowo/Github/one-attention-head-is-all-you-need/notebooks/03_VariableLength.ipynb#X36sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/randyardywibowo/Github/one-attention-head-is-all-you-need/notebooks/03_VariableLength.ipynb#X36sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m optim\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/randyardywibowo/Github/one-attention-head-is-all-you-need/notebooks/03_VariableLength.ipynb#X36sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/randyardywibowo/Github/one-attention-head-is-all-you-need/notebooks/03_VariableLength.ipynb#X36sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep(loss)\n",
            "File \u001b[0;32m~/miniconda3/envs/lit/lib/python3.9/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/lit/lib/python3.9/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
            "File \u001b[0;32m~/miniconda3/envs/lit/lib/python3.9/site-packages/torch/optim/adamw.py:171\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    161\u001b[0m         group,\n\u001b[1;32m    162\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m         state_steps,\n\u001b[1;32m    169\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m     adamw(\n\u001b[1;32m    172\u001b[0m         params_with_grad,\n\u001b[1;32m    173\u001b[0m         grads,\n\u001b[1;32m    174\u001b[0m         exp_avgs,\n\u001b[1;32m    175\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    176\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    177\u001b[0m         state_steps,\n\u001b[1;32m    178\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    179\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    180\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    181\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    182\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    183\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    184\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    185\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    186\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    187\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    188\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    189\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    190\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m~/miniconda3/envs/lit/lib/python3.9/site-packages/torch/optim/adamw.py:321\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 321\u001b[0m func(\n\u001b[1;32m    322\u001b[0m     params,\n\u001b[1;32m    323\u001b[0m     grads,\n\u001b[1;32m    324\u001b[0m     exp_avgs,\n\u001b[1;32m    325\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    326\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    327\u001b[0m     state_steps,\n\u001b[1;32m    328\u001b[0m     amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    329\u001b[0m     beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    330\u001b[0m     beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    331\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    332\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    333\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    334\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    335\u001b[0m     capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    336\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    337\u001b[0m     grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    338\u001b[0m     found_inf\u001b[39m=\u001b[39;49mfound_inf,\n\u001b[1;32m    339\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/lit/lib/python3.9/site-packages/torch/optim/adamw.py:442\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m--> 442\u001b[0m param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "history = train_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss: 0.0433\n",
            "Validation accuracy: 0.0303\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "([0.04110637679696083,\n",
              "  0.04223769158124924,\n",
              "  0.041912999004125595,\n",
              "  0.04588881507515907,\n",
              "  0.041709184646606445,\n",
              "  0.04244426637887955,\n",
              "  0.04176424816250801,\n",
              "  0.042342327535152435,\n",
              "  0.04214364290237427,\n",
              "  0.04341791942715645,\n",
              "  0.04026700556278229,\n",
              "  0.045411936938762665,\n",
              "  0.042884815484285355,\n",
              "  0.04374999925494194,\n",
              "  0.043530602008104324,\n",
              "  0.04463113844394684,\n",
              "  0.043880242854356766,\n",
              "  0.04069150984287262,\n",
              "  0.04428984597325325,\n",
              "  0.0401235856115818,\n",
              "  0.044800154864788055,\n",
              "  0.044650595635175705,\n",
              "  0.04263962805271149,\n",
              "  0.041887521743774414,\n",
              "  0.043530672788619995,\n",
              "  0.044141512364149094,\n",
              "  0.04281201213598251,\n",
              "  0.04327555000782013,\n",
              "  0.044615600258111954,\n",
              "  0.042783599346876144,\n",
              "  0.04197977855801582,\n",
              "  0.04452972859144211,\n",
              "  0.044222187250852585,\n",
              "  0.04244568198919296,\n",
              "  0.04305843263864517,\n",
              "  0.043038155883550644,\n",
              "  0.043601926416158676,\n",
              "  0.04376507177948952,\n",
              "  0.0448208786547184,\n",
              "  0.04108138009905815,\n",
              "  0.0417352095246315,\n",
              "  0.044663723558187485,\n",
              "  0.04220351204276085,\n",
              "  0.04291907325387001,\n",
              "  0.044121529906988144,\n",
              "  0.04178675636649132,\n",
              "  0.04432506859302521,\n",
              "  0.04482020437717438,\n",
              "  0.03952038660645485,\n",
              "  0.04530467465519905,\n",
              "  0.04275934025645256,\n",
              "  0.0436449833214283,\n",
              "  0.044970378279685974,\n",
              "  0.041107285767793655,\n",
              "  0.04240470007061958,\n",
              "  0.04460590332746506,\n",
              "  0.04433627426624298,\n",
              "  0.044262878596782684,\n",
              "  0.044311169534921646,\n",
              "  0.045815713703632355,\n",
              "  0.044250622391700745,\n",
              "  0.044620025902986526,\n",
              "  0.04322546347975731,\n",
              "  0.041003961116075516,\n",
              "  0.043898314237594604,\n",
              "  0.04398983716964722,\n",
              "  0.042057838290929794,\n",
              "  0.04278811439871788,\n",
              "  0.042761895805597305,\n",
              "  0.0421430729329586,\n",
              "  0.04213326796889305,\n",
              "  0.04432429373264313,\n",
              "  0.040500592440366745,\n",
              "  0.044098176062107086,\n",
              "  0.04416607320308685,\n",
              "  0.042790114879608154,\n",
              "  0.042180079966783524,\n",
              "  0.04384085536003113,\n",
              "  0.044903501868247986,\n",
              "  0.04603611305356026,\n",
              "  0.044122226536273956,\n",
              "  0.0406440868973732,\n",
              "  0.04294823110103607,\n",
              "  0.04395872354507446,\n",
              "  0.04199754074215889,\n",
              "  0.045188818126916885,\n",
              "  0.04184425249695778,\n",
              "  0.04325554892420769,\n",
              "  0.04283412918448448,\n",
              "  0.044358834624290466,\n",
              "  0.043904807418584824,\n",
              "  0.04212024062871933,\n",
              "  0.04138921946287155,\n",
              "  0.04151426628232002,\n",
              "  0.039346978068351746,\n",
              "  0.04366939514875412,\n",
              "  0.043572138994932175,\n",
              "  0.042359378188848495,\n",
              "  0.04354553297162056,\n",
              "  0.04100964590907097,\n",
              "  0.04197278991341591,\n",
              "  0.04205787554383278,\n",
              "  0.043531760573387146,\n",
              "  0.0424954779446125,\n",
              "  0.041752029210329056,\n",
              "  0.040194254368543625,\n",
              "  0.043103668838739395,\n",
              "  0.043857838958501816,\n",
              "  0.03978143259882927,\n",
              "  0.04193712770938873,\n",
              "  0.04364167898893356,\n",
              "  0.043354395776987076,\n",
              "  0.04192701727151871,\n",
              "  0.043062224984169006,\n",
              "  0.04489569738507271,\n",
              "  0.04345083609223366,\n",
              "  0.046983249485492706,\n",
              "  0.04364718496799469,\n",
              "  0.04493952542543411,\n",
              "  0.04415296018123627,\n",
              "  0.04224085807800293,\n",
              "  0.04400179162621498,\n",
              "  0.04376985877752304,\n",
              "  0.04543287307024002,\n",
              "  0.04305398836731911,\n",
              "  0.044322527945041656,\n",
              "  0.042373187839984894,\n",
              "  0.04382199048995972,\n",
              "  0.04474721476435661,\n",
              "  0.043061595410108566,\n",
              "  0.044957540929317474,\n",
              "  0.043043509125709534,\n",
              "  0.043293941766023636,\n",
              "  0.04254951328039169,\n",
              "  0.046698931604623795,\n",
              "  0.04248237982392311,\n",
              "  0.04054630175232887,\n",
              "  0.04472886025905609,\n",
              "  0.043797850608825684,\n",
              "  0.040932681411504745,\n",
              "  0.04367754980921745,\n",
              "  0.04192369431257248,\n",
              "  0.04492388293147087,\n",
              "  0.04477381706237793,\n",
              "  0.04251941666007042,\n",
              "  0.04362381994724274,\n",
              "  0.044032469391822815,\n",
              "  0.0439021997153759,\n",
              "  0.044379670172929764,\n",
              "  0.043247077614068985,\n",
              "  0.044227197766304016,\n",
              "  0.0411512553691864,\n",
              "  0.04204609617590904,\n",
              "  0.04282195866107941,\n",
              "  0.04324781894683838,\n",
              "  0.04312954470515251,\n",
              "  0.043904032558202744,\n",
              "  0.04146365076303482,\n",
              "  0.04475218057632446,\n",
              "  0.04527166113257408,\n",
              "  0.04118697717785835,\n",
              "  0.04258052632212639,\n",
              "  0.04290594533085823,\n",
              "  0.04378799349069595,\n",
              "  0.04464298486709595,\n",
              "  0.04310812056064606,\n",
              "  0.0436863973736763,\n",
              "  0.04387054219841957,\n",
              "  0.04223256930708885,\n",
              "  0.043161142617464066,\n",
              "  0.04131871089339256,\n",
              "  0.04621179774403572,\n",
              "  0.04461265355348587,\n",
              "  0.044562481343746185,\n",
              "  0.04148796573281288,\n",
              "  0.043006036430597305,\n",
              "  0.04333701357245445,\n",
              "  0.043667469173669815,\n",
              "  0.04411696270108223,\n",
              "  0.04376339912414551,\n",
              "  0.04283495247364044,\n",
              "  0.042939480394124985,\n",
              "  0.04282205551862717,\n",
              "  0.045914553105831146,\n",
              "  0.041532620787620544,\n",
              "  0.042783260345458984,\n",
              "  0.04482274875044823,\n",
              "  0.042582206428050995,\n",
              "  0.043235208839178085,\n",
              "  0.04666031897068024,\n",
              "  0.04539251700043678,\n",
              "  0.041359689086675644,\n",
              "  0.04337982088327408,\n",
              "  0.04253756254911423,\n",
              "  0.04609779268503189,\n",
              "  0.04068848863244057,\n",
              "  0.046855125576257706,\n",
              "  0.04318898171186447,\n",
              "  0.04466802626848221,\n",
              "  0.04315842688083649,\n",
              "  0.04174753278493881,\n",
              "  0.04361072555184364,\n",
              "  0.04268215224146843,\n",
              "  0.04496237263083458,\n",
              "  0.045049965381622314,\n",
              "  0.04520257189869881,\n",
              "  0.043985459953546524,\n",
              "  0.04209425672888756,\n",
              "  0.042875681072473526,\n",
              "  0.04558209329843521,\n",
              "  0.04377856105566025,\n",
              "  0.045769404619932175,\n",
              "  0.045206330716609955,\n",
              "  0.0422879196703434,\n",
              "  0.04462720453739166],\n",
              " [0.0380859375,\n",
              "  0.0400390625,\n",
              "  0.0224609375,\n",
              "  0.037109375,\n",
              "  0.03515625,\n",
              "  0.0302734375,\n",
              "  0.037109375,\n",
              "  0.0283203125,\n",
              "  0.0322265625,\n",
              "  0.025390625,\n",
              "  0.0234375,\n",
              "  0.0302734375,\n",
              "  0.02734375,\n",
              "  0.0283203125,\n",
              "  0.041015625,\n",
              "  0.041015625,\n",
              "  0.041015625,\n",
              "  0.029296875,\n",
              "  0.0263671875,\n",
              "  0.0302734375,\n",
              "  0.0244140625,\n",
              "  0.0341796875,\n",
              "  0.0234375,\n",
              "  0.033203125,\n",
              "  0.041015625,\n",
              "  0.025390625,\n",
              "  0.021484375,\n",
              "  0.0283203125,\n",
              "  0.0263671875,\n",
              "  0.0341796875,\n",
              "  0.0322265625,\n",
              "  0.03125,\n",
              "  0.0234375,\n",
              "  0.03125,\n",
              "  0.03515625,\n",
              "  0.041015625,\n",
              "  0.0224609375,\n",
              "  0.0341796875,\n",
              "  0.0361328125,\n",
              "  0.03515625,\n",
              "  0.0224609375,\n",
              "  0.021484375,\n",
              "  0.033203125,\n",
              "  0.0263671875,\n",
              "  0.0244140625,\n",
              "  0.03125,\n",
              "  0.033203125,\n",
              "  0.03125,\n",
              "  0.0302734375,\n",
              "  0.0322265625,\n",
              "  0.0341796875,\n",
              "  0.029296875,\n",
              "  0.03125,\n",
              "  0.0322265625,\n",
              "  0.0380859375,\n",
              "  0.03125,\n",
              "  0.0302734375,\n",
              "  0.029296875,\n",
              "  0.03125,\n",
              "  0.02734375,\n",
              "  0.029296875,\n",
              "  0.0302734375,\n",
              "  0.025390625,\n",
              "  0.0205078125,\n",
              "  0.0244140625,\n",
              "  0.0302734375,\n",
              "  0.0283203125,\n",
              "  0.0244140625,\n",
              "  0.0244140625,\n",
              "  0.0283203125,\n",
              "  0.0224609375,\n",
              "  0.025390625,\n",
              "  0.0302734375,\n",
              "  0.0302734375,\n",
              "  0.0234375,\n",
              "  0.02734375,\n",
              "  0.0341796875,\n",
              "  0.03515625,\n",
              "  0.0224609375,\n",
              "  0.037109375,\n",
              "  0.025390625,\n",
              "  0.0234375,\n",
              "  0.0283203125,\n",
              "  0.025390625,\n",
              "  0.02734375,\n",
              "  0.03125,\n",
              "  0.0341796875,\n",
              "  0.029296875,\n",
              "  0.0390625,\n",
              "  0.0302734375,\n",
              "  0.029296875,\n",
              "  0.0400390625,\n",
              "  0.0380859375,\n",
              "  0.033203125,\n",
              "  0.0263671875,\n",
              "  0.025390625,\n",
              "  0.0380859375,\n",
              "  0.0390625,\n",
              "  0.029296875,\n",
              "  0.033203125,\n",
              "  0.02734375,\n",
              "  0.029296875,\n",
              "  0.037109375,\n",
              "  0.033203125,\n",
              "  0.0302734375,\n",
              "  0.029296875,\n",
              "  0.03125,\n",
              "  0.03515625,\n",
              "  0.025390625,\n",
              "  0.025390625,\n",
              "  0.033203125,\n",
              "  0.029296875,\n",
              "  0.017578125,\n",
              "  0.0244140625,\n",
              "  0.0263671875,\n",
              "  0.0263671875,\n",
              "  0.02734375,\n",
              "  0.029296875,\n",
              "  0.033203125,\n",
              "  0.033203125,\n",
              "  0.0302734375,\n",
              "  0.0263671875,\n",
              "  0.025390625,\n",
              "  0.03125,\n",
              "  0.0341796875,\n",
              "  0.0283203125,\n",
              "  0.0283203125,\n",
              "  0.021484375,\n",
              "  0.03125,\n",
              "  0.0302734375,\n",
              "  0.0283203125,\n",
              "  0.0302734375,\n",
              "  0.0302734375,\n",
              "  0.033203125,\n",
              "  0.03515625,\n",
              "  0.0322265625,\n",
              "  0.0302734375,\n",
              "  0.033203125,\n",
              "  0.0322265625,\n",
              "  0.0302734375,\n",
              "  0.033203125,\n",
              "  0.021484375,\n",
              "  0.0234375,\n",
              "  0.037109375,\n",
              "  0.0380859375,\n",
              "  0.025390625,\n",
              "  0.017578125,\n",
              "  0.0400390625,\n",
              "  0.0302734375,\n",
              "  0.0283203125,\n",
              "  0.0341796875,\n",
              "  0.037109375,\n",
              "  0.025390625,\n",
              "  0.0341796875,\n",
              "  0.0283203125,\n",
              "  0.033203125,\n",
              "  0.0234375,\n",
              "  0.037109375,\n",
              "  0.0322265625,\n",
              "  0.0283203125,\n",
              "  0.0263671875,\n",
              "  0.0341796875,\n",
              "  0.029296875,\n",
              "  0.025390625,\n",
              "  0.0224609375,\n",
              "  0.041015625,\n",
              "  0.0302734375,\n",
              "  0.0341796875,\n",
              "  0.02734375,\n",
              "  0.0283203125,\n",
              "  0.033203125,\n",
              "  0.0302734375,\n",
              "  0.0283203125,\n",
              "  0.029296875,\n",
              "  0.03515625,\n",
              "  0.03515625,\n",
              "  0.0283203125,\n",
              "  0.0263671875,\n",
              "  0.02734375,\n",
              "  0.033203125,\n",
              "  0.02734375,\n",
              "  0.021484375,\n",
              "  0.0263671875,\n",
              "  0.03515625,\n",
              "  0.037109375,\n",
              "  0.03125,\n",
              "  0.03125,\n",
              "  0.0263671875,\n",
              "  0.0263671875,\n",
              "  0.0322265625,\n",
              "  0.0361328125,\n",
              "  0.0361328125,\n",
              "  0.029296875,\n",
              "  0.04296875,\n",
              "  0.0322265625,\n",
              "  0.0302734375,\n",
              "  0.033203125,\n",
              "  0.0263671875,\n",
              "  0.03515625,\n",
              "  0.03515625,\n",
              "  0.0322265625,\n",
              "  0.0341796875,\n",
              "  0.0263671875,\n",
              "  0.03125,\n",
              "  0.0322265625,\n",
              "  0.025390625,\n",
              "  0.033203125,\n",
              "  0.033203125,\n",
              "  0.0185546875,\n",
              "  0.03125,\n",
              "  0.02734375,\n",
              "  0.033203125,\n",
              "  0.0302734375,\n",
              "  0.0283203125,\n",
              "  0.034722222222222224])"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def validate_all(val_loader):\n",
        "    \n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "    \n",
        "    for batch in val_loader:\n",
        "        val_data = batch[0].to(device=DEVICE)\n",
        "        val_acc = validate(model, val_data)\n",
        "        val_loss = loss_fn(model(val_data), val_data)\n",
        "        \n",
        "        val_losses.append(val_loss.item())\n",
        "        val_accs.append(val_acc)\n",
        "    \n",
        "    print(f\"Validation loss: {np.mean(val_losses):.4f}\")\n",
        "    print(f\"Validation accuracy: {np.mean(val_accs):.4f}\")\n",
        "    \n",
        "    return val_losses, val_accs\n",
        "\n",
        "validate_all(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([[4, 1, 3,  ..., 3, 3, 3],\n",
              "         [4, 1, 2,  ..., 2, 2, 2],\n",
              "         [4, 2, 1,  ..., 3, 3, 3],\n",
              "         ...,\n",
              "         [4, 3, 0,  ..., 3, 3, 3],\n",
              "         [4, 3, 1,  ..., 3, 3, 3],\n",
              "         [4, 2, 3,  ..., 3, 3, 3]])]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(val_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HABWttcH34dv"
      },
      "source": [
        "### Testing post-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lakshPyuwdO7",
        "outputId": "259e2ca7-0398-476f-f6ba-094006179a44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating on validation data:\n",
            "25\n",
            "tensor([[0, 0, 0,  ..., 2, 2, 2],\n",
            "        [0, 0, 0,  ..., 2, 2, 2],\n",
            "        [1, 1, 1,  ..., 2, 2, 2],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 3, 3, 3],\n",
            "        [0, 0, 0,  ..., 2, 2, 2],\n",
            "        [1, 1, 1,  ..., 3, 3, 3]])\n",
            "tensor([[0, 0, 0,  ..., 2, 2, 2],\n",
            "        [0, 0, 0,  ..., 2, 2, 2],\n",
            "        [1, 1, 1,  ..., 2, 2, 2],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 3, 3, 3],\n",
            "        [0, 0, 0,  ..., 2, 2, 2],\n",
            "        [1, 1, 1,  ..., 3, 3, 3]])\n",
            "\tval_acc=99.023%\n",
            "\n",
            "[65] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1] | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
            "[79] [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3, 3, 3, 3, 3] | [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3]\n",
            "[281] [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3] | [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "[297] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1] | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
            "[503] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1] | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
            "[627] [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3] | [0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3]\n",
            "[753] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1] | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
            "[754] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1] | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
            "[798] [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2] | [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "[935] [0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3] | [0, 0, 0, 0, 0, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "10/1024 (0.98%)\n",
            "\n",
            "Validating on test data:\n",
            "25\n",
            "tensor([[1, 1, 1,  ..., 2, 2, 2],\n",
            "        [1, 1, 1,  ..., 3, 3, 3],\n",
            "        [1, 1, 1,  ..., 3, 3, 3],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 2, 2, 2],\n",
            "        [1, 1, 1,  ..., 3, 3, 3],\n",
            "        [0, 0, 0,  ..., 3, 3, 3]])\n",
            "tensor([[1, 1, 1,  ..., 2, 2, 2],\n",
            "        [1, 1, 1,  ..., 3, 3, 3],\n",
            "        [1, 1, 1,  ..., 3, 3, 3],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 2, 2, 2],\n",
            "        [1, 1, 1,  ..., 3, 3, 3],\n",
            "        [0, 0, 0,  ..., 3, 3, 3]])\n",
            "\ttest_acc=98.535%\n",
            "\n",
            "[11] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3, 3, 3] | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3]\n",
            "[54] [0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3] | [0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3]\n",
            "[84] [2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3] | [2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "[101] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2] | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2]\n",
            "[208] [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3] | [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "[217] [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3] | [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "[292] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1] | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
            "[438] [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3, 3, 3] | [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3]\n",
            "[449] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1] | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
            "[477] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1] | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
            "[567] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1] | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
            "[882] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3] | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3]\n",
            "[892] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1] | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
            "[950] [0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3] | [0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "[966] [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3] | [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "15/1024 (1.46%)\n"
          ]
        }
      ],
      "source": [
        "print(\"Validating on validation data:\")\n",
        "val_batch = next(iter(val_loader))[0]\n",
        "val_acc = validate(model, val_batch)\n",
        "print(f\"\\t{val_acc=:.3%}\\n\")\n",
        "if val_acc < 1:\n",
        "    show_mispreds(model, val_batch)\n",
        "\n",
        "test_batch = next(iter(test_loader))[0]\n",
        "print(\"\\nValidating on test data:\")\n",
        "test_acc = validate(model, test_batch)\n",
        "print(f\"\\t{test_acc=:.3%}\\n\")\n",
        "if test_acc < 1:\n",
        "    show_mispreds(model, test_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNofXtfSQvh2"
      },
      "source": [
        "### Saving trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "i9lLO9p1d8mC"
      },
      "outputs": [],
      "source": [
        "def save_model_state_dict(\n",
        "    model: HookedTransformer, \n",
        "    filename: str | None = None\n",
        ") -> None:\n",
        "    if not os.path.isdir(\"models\"):\n",
        "        os.mkdir(\"models\")\n",
        "    if not filename:\n",
        "        timestamp = dt.now().isoformat(\"T\", \"minutes\").replace(\":\", \"-\")\n",
        "        filename = f\"model_state_dict_{timestamp}.pkl\"\n",
        "    with open(f\"models/{filename}\", \"wb\") as f:\n",
        "        pickle.dump(model.state_dict(), f)\n",
        "\n",
        "save_model_state_dict(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "attn = model.blocks[0].attn\n",
        "all_token_embeddings = model.embed(range(D_VOCAB))\n",
        "\n",
        "embeddings_key = einsum(\"d_vocab d_model, n_heads d_model d_head -> n_heads d_vocab d_head\", \n",
        "                        all_token_embeddings, attn.W_K) \n",
        "embeddings_query = einsum(\"d_vocab d_model, n_heads d_model d_head -> n_heads d_vocab d_head\", \n",
        "                          all_token_embeddings, attn.W_Q) \n",
        "\n",
        "plt.rcParams['figure.figsize'] = [20, 10]\n",
        "qk_circuit_attn_heatmap = einsum(\n",
        "    \"n_heads d_vocab_q d_head, n_heads d_vocab_k d_head -> ... d_vocab_q d_vocab_k\", \n",
        "    embeddings_query, embeddings_key\n",
        "    ).detach().cpu().numpy()\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "graph = ax.imshow(qk_circuit_attn_heatmap, cmap=\"hot\", interpolation=\"nearest\")\n",
        "plt.colorbar(graph)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_tokens = torch.Tensor([START_TOKEN_ID] + [0] * 5 + [1] * 5 + [MID_TOKEN_ID] + 5 * [0] + 5 * [1]).long().to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_tokens \n",
        "pos_embeds = model.pos_embed(input_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([48, 128])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.pos_embed.W_pos.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import *\n",
        "from transformer_lens import ActivationCache, HookedTransformer\n",
        "from jaxtyping import Float\n",
        "from circuitsvis.attention import attention_heads\n",
        "\n",
        "def sharpen_probabilities(tensor, temperature):\n",
        "    # Make sure the temperature is greater than 0 to avoid division by zero\n",
        "    assert temperature > 0, \"Temperature must be greater than 0\"\n",
        "    \n",
        "    # Sharpen the probabilities\n",
        "    sharpened = torch.pow(tensor, 1/temperature)\n",
        "    \n",
        "    # Renormalize the rows to sum to 1\n",
        "    row_sums = sharpened.sum(dim=1).unsqueeze(1)\n",
        "    normalized = sharpened / row_sums\n",
        "    \n",
        "    return normalized\n",
        "\n",
        "def visualize_attention_patterns(\n",
        "    heads: Union[List[int], int, Float[torch.Tensor, \"heads\"]],\n",
        "    local_cache: ActivationCache,\n",
        "    local_tokens: torch.Tensor,\n",
        "    title: Optional[str] = \"\",\n",
        "    max_width: Optional[int] = 1000,\n",
        "    temp = 0.9\n",
        ") -> str:\n",
        "    # If a single head is given, convert to a list\n",
        "    if isinstance(heads, int):\n",
        "        heads = [heads]\n",
        "\n",
        "    # Create the plotting data\n",
        "    labels: List[str] = []\n",
        "    patterns: List[Float[torch.Tensor, \"dest_pos src_pos\"]] = []\n",
        "\n",
        "    # Assume we have a single batch item\n",
        "    batch_index = 0\n",
        "\n",
        "    for head in heads:\n",
        "        # Set the label\n",
        "        layer = head // model.cfg.n_heads\n",
        "        head_index = head % model.cfg.n_heads\n",
        "        labels.append(f\"L{layer}H{head_index}\")\n",
        "\n",
        "        # Get the attention patterns for the head\n",
        "        # Attention patterns have shape [batch, head_index, query_pos, key_pos]\n",
        "        patterns.append(sharpen_probabilities(local_cache[\"attn\", layer][batch_index, head_index], temp))\n",
        "\n",
        "    # Convert the tokens to strings (for the axis labels)\n",
        "    str_tokens = local_tokens.detach().cpu().numpy().astype(str).tolist()\n",
        "\n",
        "    # Combine the patterns into a single tensor\n",
        "    patterns: Float[torch.Tensor, \"head_index dest_pos src_pos\"] = torch.stack(\n",
        "        patterns, dim=0\n",
        "    )\n",
        "\n",
        "    # Circuitsvis Plot (note we get the code version so we can concatenate with the title)\n",
        "    plot = attention_heads(\n",
        "        attention=patterns, tokens=str_tokens, attention_head_names=labels\n",
        "    ).show_code()\n",
        "\n",
        "    # Display the title\n",
        "    title_html = f\"<h2>{title}</h2><br/>\"\n",
        "\n",
        "    # Return the visualisation as raw code\n",
        "    return f\"<div style='max-width: {str(max_width)}px;'>{title_html + plot}</div>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([48])\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style='max-width: 1000px;'><h2>What</h2><br/><div id=\"circuits-vis-9d138056-fab6\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, AttentionHeads } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-9d138056-fab6\",\n",
              "      AttentionHeads,\n",
              "      {\"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2138020247220993, 0.7861979603767395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.057514481246471405, 0.5519309639930725, 0.3905545473098755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04891200736165047, 0.26455801725387573, 0.19695577025413513, 0.48957425355911255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03249381110072136, 0.3121345043182373, 0.18692727386951447, 0.2425408661365509, 0.22590357065200806, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007744737435132265, 0.23225551843643188, 0.10715993493795395, 0.2556758522987366, 0.18590837717056274, 0.21125563979148865, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.005190749652683735, 0.14088629186153412, 0.13727550208568573, 0.19309751689434052, 0.11704391986131668, 0.253492146730423, 0.15301388502120972, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.013225020840764046, 0.15127938985824585, 0.11959832161664963, 0.1173204779624939, 0.1426960825920105, 0.17412498593330383, 0.15214243531227112, 0.12961329519748688, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.013591828756034374, 0.1882787048816681, 0.10075060278177261, 0.08595497906208038, 0.12294725328683853, 0.11870162934064865, 0.12885302305221558, 0.10094670206308365, 0.13997526466846466, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006275112275034189, 0.13812066614627838, 0.07931248098611832, 0.130990669131279, 0.12380241602659225, 0.13238553702831268, 0.11755070835351944, 0.08006053417921066, 0.08865738660097122, 0.10284453630447388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.013090292923152447, 0.11746793240308762, 0.06101800501346588, 0.07355391979217529, 0.08056379854679108, 0.11887674778699875, 0.13244721293449402, 0.08630479872226715, 0.10163364559412003, 0.0705994963645935, 0.14444419741630554, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006465565413236618, 0.09719778597354889, 0.07808803766965866, 0.12069229781627655, 0.07349874824285507, 0.1104808896780014, 0.10966980457305908, 0.08350657671689987, 0.07706011831760406, 0.07409459352493286, 0.08802924305200577, 0.08121634274721146, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002453078981488943, 0.08252418041229248, 0.04074423760175705, 0.14390218257904053, 0.07112649828195572, 0.08766141533851624, 0.1168271079659462, 0.0630708560347557, 0.0684843584895134, 0.05900174006819725, 0.05779768154025078, 0.10942307114601135, 0.0969836488366127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01132967509329319, 0.05299196019768715, 0.05175384506583214, 0.09380652010440826, 0.057137757539749146, 0.11190908402204514, 0.10685563832521439, 0.06184259429574013, 0.07837315648794174, 0.056948982179164886, 0.08835121989250183, 0.10717958211898804, 0.06666088849306107, 0.05485911667346954, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04826134443283081, 0.06951531022787094, 0.0526791550219059, 0.11655615270137787, 0.06221714988350868, 0.07435786724090576, 0.11821914464235306, 0.03808522969484329, 0.05352121591567993, 0.04606207087635994, 0.08915866166353226, 0.0946262925863266, 0.0607416033744812, 0.05029233545064926, 0.025706522166728973, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002892161952331662, 0.054513562470674515, 0.05089198052883148, 0.05660504102706909, 0.06446272879838943, 0.09282905608415604, 0.08367183059453964, 0.07380730658769608, 0.07029556483030319, 0.0756310299038887, 0.05852653458714485, 0.07081194967031479, 0.07841649651527405, 0.045919135212898254, 0.05862116441130638, 0.06210445985198021, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.014094053767621517, 0.055038489401340485, 0.039138805121183395, 0.0764753445982933, 0.057696714997291565, 0.07585372775793076, 0.08805188536643982, 0.06159603223204613, 0.04802245646715164, 0.050965745002031326, 0.11326846480369568, 0.09590110927820206, 0.06222985312342644, 0.03700139746069908, 0.035823021084070206, 0.060854580253362656, 0.027988363057374954, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0325280986726284, 0.04728325828909874, 0.05006571114063263, 0.0798785537481308, 0.04753816872835159, 0.0943848267197609, 0.07330451905727386, 0.06889202445745468, 0.050862882286310196, 0.049838386476039886, 0.0710061639547348, 0.06096452847123146, 0.05036889389157295, 0.040815357118844986, 0.04002349451184273, 0.04159032925963402, 0.035008084028959274, 0.06564673036336899, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006162034347653389, 0.060039862990379333, 0.04892585054039955, 0.06946470588445663, 0.055730219930410385, 0.06234164163470268, 0.05475858226418495, 0.04666854813694954, 0.0470442995429039, 0.0432891845703125, 0.06607455760240555, 0.050274886190891266, 0.06349501758813858, 0.04806649312376976, 0.052707117050886154, 0.04558245837688446, 0.04969198629260063, 0.05831444635987282, 0.07136813551187515, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004258335568010807, 0.050474636256694794, 0.038464222103357315, 0.10691487789154053, 0.046263281255960464, 0.07746809720993042, 0.0748131051659584, 0.051332246512174606, 0.04795248806476593, 0.05335394665598869, 0.02879049815237522, 0.05507498234510422, 0.06253526359796524, 0.0248852651566267, 0.04019180312752724, 0.02964073419570923, 0.026132993400096893, 0.08441625535488129, 0.05734127387404442, 0.03969570994377136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.020602349191904068, 0.042534880340099335, 0.03160000964999199, 0.07062548398971558, 0.04914288967847824, 0.0504501648247242, 0.09159296751022339, 0.03362353518605232, 0.04463546350598335, 0.040531910955905914, 0.04853765293955803, 0.06742242723703384, 0.057372190058231354, 0.034487806260585785, 0.04058733209967613, 0.06986598670482635, 0.025516826659440994, 0.056439295411109924, 0.0392780601978302, 0.053588178008794785, 0.03156459331512451, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015262975357472897, 0.048005610704422, 0.04897920414805412, 0.049481336027383804, 0.048418208956718445, 0.05014350265264511, 0.05034796893596649, 0.046242788434028625, 0.05267084762454033, 0.05119621381163597, 0.04634734243154526, 0.04822231084108353, 0.051845282316207886, 0.03537655249238014, 0.04696175083518028, 0.052475456148386, 0.05063050240278244, 0.06471177190542221, 0.04368547350168228, 0.046562470495700836, 0.03559514880180359, 0.01683727838099003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03894907981157303, 0.05580553784966469, 0.027508633211255074, 0.03034438192844391, 0.038207150995731354, 0.04686270281672478, 0.039808761328458786, 0.03962049260735512, 0.04329768568277359, 0.032622646540403366, 0.052902061492204666, 0.04828266054391861, 0.05120077356696129, 0.035623352974653244, 0.032957833260297775, 0.05140247195959091, 0.03932832181453705, 0.028173547238111496, 0.04834494739770889, 0.06747224926948547, 0.028141185641288757, 0.063303641974926, 0.059839874505996704, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.053813230246305466, 0.03776135668158531, 0.032427821308374405, 0.06914138793945312, 0.03327826038002968, 0.046048011630773544, 0.054994817823171616, 0.0305384062230587, 0.03785530477762222, 0.03356779366731644, 0.05980660766363144, 0.05060126632452011, 0.04811500012874603, 0.029139181599020958, 0.025896398350596428, 0.047012366354465485, 0.022485284134745598, 0.06620071828365326, 0.041781697422266006, 0.02995983511209488, 0.024208486080169678, 0.039886217564344406, 0.04976698383688927, 0.035713568329811096, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00010246492456644773, 0.04613484442234039, 0.0483928881585598, 0.0596800297498703, 0.05018661916255951, 0.04124736413359642, 0.05624846741557121, 0.05313204228878021, 0.04803074523806572, 0.040245603770017624, 0.0599546916782856, 0.04391142353415489, 0.05522593483328819, 0.030873537063598633, 0.03744612634181976, 0.045886874198913574, 0.03729698434472084, 0.04872087016701698, 0.04660307243466377, 0.04958299547433853, 0.052104175090789795, 0.002946580294519663, 0.0035495543852448463, 0.0031303868163377047, 0.03936568647623062, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1536579430103302, 0.033178988844156265, 0.0349336639046669, 0.03715607523918152, 0.0365939699113369, 0.040552347898483276, 0.04559227451682091, 0.031156955286860466, 0.03649284690618515, 0.03186536580324173, 0.035557642579078674, 0.027458131313323975, 0.04142635315656662, 0.03436937555670738, 0.036579471081495285, 0.036599814891815186, 0.04347736015915871, 0.04791532829403877, 0.041642822325229645, 0.03954368084669113, 0.043802693486213684, 0.006371792871505022, 0.007286600768566132, 0.007119500078260899, 0.05819583311676979, 0.011473246850073338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00043404431198723614, 0.05083076283335686, 0.04102080315351486, 0.04872337728738785, 0.04924754798412323, 0.03982304036617279, 0.05200086906552315, 0.046433109790086746, 0.040209509432315826, 0.03884245827794075, 0.04726824164390564, 0.039605919271707535, 0.05038008093833923, 0.03775323927402496, 0.04682031273841858, 0.05377873033285141, 0.03999767452478409, 0.05154236778616905, 0.0423743762075901, 0.049121059477329254, 0.05053681880235672, 0.003999433945864439, 0.004681879188865423, 0.005179240833967924, 0.061209484934806824, 0.0033165873028337955, 0.004869026131927967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0010999491205438972, 0.046572424471378326, 0.04143206402659416, 0.04406558349728584, 0.04811755195260048, 0.041999295353889465, 0.04542269930243492, 0.04869905114173889, 0.04294908791780472, 0.041627027094364166, 0.04731657728552818, 0.04235250502824783, 0.047113824635744095, 0.038226641714572906, 0.043711431324481964, 0.04972174018621445, 0.04402211308479309, 0.046631623059511185, 0.043404027819633484, 0.049475379288196564, 0.05117296800017357, 0.005592916626483202, 0.006311672739684582, 0.007007838226854801, 0.06588675081729889, 0.003195770550519228, 0.0037128347903490067, 0.0031586247496306896, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0013184476410970092, 0.04372144117951393, 0.041585035622119904, 0.04324352741241455, 0.048161983489990234, 0.04325011745095253, 0.043526433408260345, 0.04724743589758873, 0.043599046766757965, 0.04171227291226387, 0.04551210626959801, 0.04319847375154495, 0.04366772249341011, 0.039199329912662506, 0.04475030303001404, 0.047223929315805435, 0.04518909752368927, 0.04374909773468971, 0.04162187501788139, 0.04704839363694191, 0.04873053729534149, 0.007209640461951494, 0.00762174790725112, 0.008195561356842518, 0.07606624811887741, 0.0023774493020027876, 0.0030384361743927, 0.0026839845813810825, 0.005550311878323555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0014852877939119935, 0.0422806441783905, 0.04206616431474686, 0.043025124818086624, 0.046130646020174026, 0.043854743242263794, 0.04084192216396332, 0.045545272529125214, 0.04159758612513542, 0.04053732752799988, 0.04417729377746582, 0.04255499690771103, 0.04233789071440697, 0.04018416628241539, 0.0446704626083374, 0.04606787487864494, 0.04271808639168739, 0.042766131460666656, 0.04218526557087898, 0.04362596571445465, 0.04896033927798271, 0.008711780421435833, 0.008846702054142952, 0.009008694440126419, 0.08845771849155426, 0.002212120220065117, 0.0024526622146368027, 0.0023764390498399734, 0.004768907558172941, 0.005551782436668873, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00246665021404624, 0.042089566588401794, 0.04086003080010414, 0.04281041398644447, 0.043501339852809906, 0.04390806332230568, 0.04034843295812607, 0.04386370629072189, 0.041793592274188995, 0.04036692902445793, 0.04341670498251915, 0.04147278144955635, 0.043803296983242035, 0.03950778767466545, 0.044251151382923126, 0.043433137238025665, 0.04140184447169304, 0.04186803847551346, 0.04360620677471161, 0.046326894313097, 0.04688761755824089, 0.009493335150182247, 0.009910565800964832, 0.010366112925112247, 0.08238062262535095, 0.003138501662760973, 0.0035510787274688482, 0.004814229905605316, 0.006791559047996998, 0.006180189084261656, 0.0053895944729447365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0024743361864238977, 0.040417369455099106, 0.04080364480614662, 0.04140649735927582, 0.04306004196405411, 0.04323877394199371, 0.04069152846932411, 0.04400313273072243, 0.04174943268299103, 0.03949735686182976, 0.0429111048579216, 0.04053835570812225, 0.04234350472688675, 0.03896382451057434, 0.044390082359313965, 0.04167618975043297, 0.04050084576010704, 0.04256488382816315, 0.04193777218461037, 0.04323124140501022, 0.04642939567565918, 0.00984872505068779, 0.010193913243710995, 0.010016676969826221, 0.08096467703580856, 0.0036032740026712418, 0.0032802103087306023, 0.005464477464556694, 0.00839559268206358, 0.008601592853665352, 0.0065895188599824905, 0.010212025605142117, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0025035403668880463, 0.04114877060055733, 0.04024624451994896, 0.0412730798125267, 0.041340172290802, 0.04121589660644531, 0.04043137654662132, 0.04176335781812668, 0.0418170690536499, 0.03927192464470863, 0.04246607795357704, 0.04121639207005501, 0.04326683655381203, 0.038679614663124084, 0.04220899939537048, 0.042907435446977615, 0.04094301536679268, 0.04218737408518791, 0.04018774628639221, 0.04286094009876251, 0.045697372406721115, 0.0097197275608778, 0.010043440386652946, 0.00994389969855547, 0.07044590264558792, 0.0037194148171693087, 0.0034214118495583534, 0.006479276344180107, 0.00965942069888115, 0.008026141673326492, 0.008734481409192085, 0.011187897063791752, 0.014985772781074047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0033963953610509634, 0.040515825152397156, 0.04155971109867096, 0.04048316180706024, 0.042309220880270004, 0.0429755337536335, 0.04069417715072632, 0.04143230989575386, 0.04145068675279617, 0.03973499312996864, 0.042008139193058014, 0.04083159193396568, 0.04098483547568321, 0.0392414927482605, 0.04229476675391197, 0.04015014320611954, 0.0406317301094532, 0.04096603021025658, 0.03991921618580818, 0.042107872664928436, 0.04487476870417595, 0.010653437115252018, 0.011011366732418537, 0.01084996946156025, 0.06039819121360779, 0.005526806693524122, 0.004812869243323803, 0.007382824085652828, 0.009373550303280354, 0.010095558129251003, 0.008088953793048859, 0.009864086285233498, 0.011747948825359344, 0.01163184829056263, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0036066158208996058, 0.04079612344503403, 0.04108225554227829, 0.03939860314130783, 0.04192720726132393, 0.04154691472649574, 0.039094407111406326, 0.041614264249801636, 0.03991447761654854, 0.039336394518613815, 0.0417063944041729, 0.039025332778692245, 0.0408647246658802, 0.03777587041258812, 0.042332783341407776, 0.039890021085739136, 0.03935215249657631, 0.03952254727482796, 0.04019520431756973, 0.042228538542985916, 0.04346555471420288, 0.011044801212847233, 0.011362881399691105, 0.01153262797743082, 0.05552088841795921, 0.006576883140951395, 0.004769463092088699, 0.008844232186675072, 0.010379952378571033, 0.011173324659466743, 0.007968331687152386, 0.009380479343235493, 0.01208181120455265, 0.013104678131639957, 0.011583271436393261, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004515983164310455, 0.040405839681625366, 0.03884545713663101, 0.03966638073325157, 0.0402357317507267, 0.04207709804177284, 0.03875144198536873, 0.039536088705062866, 0.03947119414806366, 0.03792403265833855, 0.041607484221458435, 0.03961016982793808, 0.04168197140097618, 0.03834499791264534, 0.04217405244708061, 0.03811798617243767, 0.039412010461091995, 0.0396420955657959, 0.03949667513370514, 0.04016369208693504, 0.04391653463244438, 0.010866208001971245, 0.011160284280776978, 0.010833049193024635, 0.04722537472844124, 0.006955145392566919, 0.005361015442758799, 0.008882380090653896, 0.010645897127687931, 0.011611527763307095, 0.009376601316034794, 0.01166541874408722, 0.011669150553643703, 0.012652397155761719, 0.011274191550910473, 0.014224489219486713, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.005407131277024746, 0.03939874470233917, 0.039445508271455765, 0.038900066167116165, 0.04006700590252876, 0.04026662930846214, 0.03777572512626648, 0.03862961381673813, 0.03964431956410408, 0.038556504994630814, 0.03894354775547981, 0.03920561075210571, 0.040169063955545425, 0.03746531158685684, 0.04095761105418205, 0.03700779750943184, 0.0384540893137455, 0.03798232600092888, 0.03899116814136505, 0.0402507558465004, 0.04326784238219261, 0.009962928481400013, 0.010119462385773659, 0.010269148275256157, 0.03970988839864731, 0.0073578315787017345, 0.005316482856869698, 0.01025144662708044, 0.012813849374651909, 0.012407571077346802, 0.011060096323490143, 0.011920008808374405, 0.012875319458544254, 0.014397846534848213, 0.01033189706504345, 0.014083484187722206, 0.016336413100361824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.005450798664242029, 0.03950230032205582, 0.03713984414935112, 0.036712802946567535, 0.040042608976364136, 0.040341444313526154, 0.038492366671562195, 0.03925427794456482, 0.03974546864628792, 0.03749362751841545, 0.03878113627433777, 0.040179114788770676, 0.039579667150974274, 0.03737303987145424, 0.03967950493097305, 0.037062108516693115, 0.036844175308942795, 0.038365624845027924, 0.037613432854413986, 0.04005108028650284, 0.042514655739068985, 0.009469211101531982, 0.009475878439843655, 0.00932610034942627, 0.03649609163403511, 0.007994321174919605, 0.005207194946706295, 0.011133283376693726, 0.012088493444025517, 0.013044142164289951, 0.012245376594364643, 0.010481326840817928, 0.013478043489158154, 0.01331242360174656, 0.009544865228235722, 0.01289951428771019, 0.014165488071739674, 0.01741911843419075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.005310131702572107, 0.03830765187740326, 0.038536861538887024, 0.03749058023095131, 0.03842095658183098, 0.03928958252072334, 0.036608852446079254, 0.038880180567502975, 0.03790714591741562, 0.03712889552116394, 0.039372701197862625, 0.03875008970499039, 0.03965737670660019, 0.03557141125202179, 0.03822769597172737, 0.036909330636262894, 0.03769577294588089, 0.036773040890693665, 0.037031322717666626, 0.03890332579612732, 0.04009709879755974, 0.009023783728480339, 0.009102536365389824, 0.00936877354979515, 0.03089519403874874, 0.008326503448188305, 0.004904043860733509, 0.011367562226951122, 0.012665499933063984, 0.01301917526870966, 0.013409611769020557, 0.010964008048176765, 0.012862027622759342, 0.013658243231475353, 0.011525331065058708, 0.015176950953900814, 0.016107693314552307, 0.019366905093193054, 0.011386184953153133, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004447331186383963, 0.03684936836361885, 0.037127405405044556, 0.035893477499485016, 0.03861045092344284, 0.03887712582945824, 0.03591699153184891, 0.036356206983327866, 0.03822420537471771, 0.035059571266174316, 0.03630872070789337, 0.036795906722545624, 0.03675738349556923, 0.03475742042064667, 0.0378587506711483, 0.035292915999889374, 0.03569890186190605, 0.03753714635968208, 0.03756917268037796, 0.03751083463430405, 0.038935672491788864, 0.009442662820219994, 0.009592446498572826, 0.009643619880080223, 0.029899517074227333, 0.009693796746432781, 0.005391084123402834, 0.013499747030436993, 0.011522755958139896, 0.013716849498450756, 0.013585657812654972, 0.01043800637125969, 0.012317738495767117, 0.014175277203321457, 0.012585687451064587, 0.015128565020859241, 0.016910649836063385, 0.017597578465938568, 0.011289093643426895, 0.021184347569942474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006310269236564636, 0.03545264154672623, 0.035505782812833786, 0.03541180118918419, 0.036886293441057205, 0.03751607611775398, 0.03472183644771576, 0.03556007519364357, 0.0358123704791069, 0.034524254500865936, 0.037167347967624664, 0.035985324531793594, 0.03634289279580116, 0.033496394753456116, 0.03806496784090996, 0.03412548825144768, 0.035223037004470825, 0.03547348454594612, 0.03535075858235359, 0.0351124182343483, 0.037433378398418427, 0.010149468667805195, 0.010200664401054382, 0.010588478296995163, 0.024002423509955406, 0.008392157964408398, 0.005163996014744043, 0.014252611435949802, 0.012278103269636631, 0.013312023133039474, 0.012379223480820656, 0.010365603491663933, 0.013243678957223892, 0.014661593362689018, 0.009522593580186367, 0.016283472999930382, 0.016057495027780533, 0.016055505722761154, 0.012300546281039715, 0.01907922513782978, 0.03023419715464115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00694252597168088, 0.03413020819425583, 0.03427641838788986, 0.03515428677201271, 0.03515671193599701, 0.037468016147613525, 0.03356493264436722, 0.03309261053800583, 0.034313712269067764, 0.03411393240094185, 0.03510619327425957, 0.03534601256251335, 0.03428804874420166, 0.03188341483473778, 0.03387869894504547, 0.03330802917480469, 0.03405794873833656, 0.03580298647284508, 0.03417770192027092, 0.034378860145807266, 0.03462161496281624, 0.01635259948670864, 0.01595422439277172, 0.016933623701334, 0.02754286304116249, 0.009703884832561016, 0.005438460968434811, 0.014986440539360046, 0.00990481860935688, 0.0114638926461339, 0.013615538366138935, 0.011679538525640965, 0.012118971906602383, 0.013218740001320839, 0.012405174784362316, 0.016117922961711884, 0.015214351937174797, 0.016664747148752213, 0.009887312538921833, 0.014920737594366074, 0.021539701148867607, 0.019273577257990837, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009396275505423546, 0.03298602253198624, 0.03331410139799118, 0.03359731659293175, 0.032211627811193466, 0.03391697630286217, 0.029512053355574608, 0.03278829902410507, 0.03386538103222847, 0.03179348260164261, 0.03273177146911621, 0.03342406079173088, 0.03174976631999016, 0.03166127949953079, 0.03382430225610733, 0.03263484314084053, 0.03355669230222702, 0.03545643389225006, 0.03309249132871628, 0.030443955212831497, 0.03338591009378433, 0.026088377460837364, 0.024154294282197952, 0.025396300479769707, 0.028681648895144463, 0.011212775483727455, 0.006040217820554972, 0.015611518174409866, 0.008126313798129559, 0.01342866849154234, 0.014807667583227158, 0.009915574453771114, 0.011224552989006042, 0.015692004933953285, 0.013813837431371212, 0.013640638440847397, 0.015248479321599007, 0.014541947282850742, 0.008945481851696968, 0.014506169594824314, 0.017633726820349693, 0.011350645683705807, 0.014596153050661087, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01790105365216732, 0.03077806904911995, 0.029902881011366844, 0.03150143846869469, 0.029193080961704254, 0.0325934924185276, 0.02903129532933235, 0.02944450080394745, 0.029394904151558876, 0.02907819114625454, 0.031580597162246704, 0.0318574421107769, 0.02836889587342739, 0.02871744893491268, 0.029935846105217934, 0.030902154743671417, 0.030568867921829224, 0.03418745845556259, 0.03242093697190285, 0.02968323417007923, 0.030399693176150322, 0.0365339070558548, 0.03299450874328613, 0.03473319113254547, 0.02853703312575817, 0.008964539505541325, 0.0031899376772344112, 0.014028677716851234, 0.00894215703010559, 0.01247621513903141, 0.01818901114165783, 0.006653154734522104, 0.008595315739512444, 0.01750027760863304, 0.013075259514153004, 0.01256150659173727, 0.015279002487659454, 0.02274390682578087, 0.008516660891473293, 0.00947605725377798, 0.010616530664265156, 0.00975396391004324, 0.012293845415115356, 0.02690383419394493, 0.0, 0.0, 0.0, 0.0], [0.03568631038069725, 0.0285747479647398, 0.025445668026804924, 0.0291983000934124, 0.025334812700748444, 0.027541423216462135, 0.023897366598248482, 0.02555030770599842, 0.024546734988689423, 0.025120897218585014, 0.028592178598046303, 0.027303127571940422, 0.024306919425725937, 0.026145298033952713, 0.02712014690041542, 0.02721637673676014, 0.028219064697623253, 0.030213618651032448, 0.02882998436689377, 0.022762060165405273, 0.02346636913716793, 0.06937112659215927, 0.05998167023062706, 0.07137108594179153, 0.03361225500702858, 0.005683950148522854, 0.002094370312988758, 0.009711095131933689, 0.0057326811365783215, 0.009749522432684898, 0.016252988949418068, 0.006018937565386295, 0.004114277195185423, 0.018924936652183533, 0.0085048358887434, 0.006555245723575354, 0.011215981096029282, 0.02288972958922386, 0.007506357505917549, 0.006280219182372093, 0.005928418133407831, 0.004394699353724718, 0.009337483905255795, 0.02405557595193386, 0.015640871599316597, 0.0, 0.0, 0.0], [0.05891525000333786, 0.00901744607836008, 0.006867501884698868, 0.008747508749365807, 0.00708986259996891, 0.007269479334354401, 0.007007252890616655, 0.006435913499444723, 0.006496782414615154, 0.008186661638319492, 0.009221947751939297, 0.00804845616221428, 0.008875873871147633, 0.008247976191341877, 0.0069709173403680325, 0.010527567937970161, 0.008784694597125053, 0.009410325437784195, 0.008816331624984741, 0.0084994500502944, 0.006350285839289427, 0.029774131253361702, 0.03614338859915733, 0.0401165597140789, 0.5991541743278503, 0.0043211765587329865, 0.0013736181426793337, 0.005642182659357786, 0.0009724706178531051, 0.001051687984727323, 0.002556429710239172, 0.0008516309317201376, 0.0005260694888420403, 0.002779411617666483, 0.002007364761084318, 0.0024772307369858027, 0.0020276042632758617, 0.004234755411744118, 0.0010644335998222232, 0.0010833351407200098, 0.0007856635493226349, 0.0016481594648212194, 0.0016878703609108925, 0.0036635929718613625, 0.005188263952732086, 0.029081355780363083, 0.0, 0.0], [0.11369041353464127, 0.011831752955913544, 0.00875614769756794, 0.009568038396537304, 0.007897980511188507, 0.010707021690905094, 0.007316193543374538, 0.007669977843761444, 0.010489915497601032, 0.009125298820436, 0.008048409596085548, 0.010958237573504448, 0.008885766379535198, 0.011049480177462101, 0.0072090113535523415, 0.00802141148597002, 0.013233485631644726, 0.01220078393816948, 0.012741591781377792, 0.008620649576187134, 0.010627473704516888, 0.05230050906538963, 0.04773256927728653, 0.03666672483086586, 0.3485347628593445, 0.005536823999136686, 0.001776018994860351, 0.0068187415599823, 0.0013546720147132874, 0.0016034996369853616, 0.003932628780603409, 0.0016305259196087718, 0.00102595507632941, 0.005488119553774595, 0.0026272376999258995, 0.0032144689466804266, 0.0021260057110339403, 0.0030495328828692436, 0.002540742978453636, 0.0034477519802749157, 0.002706712344661355, 0.0037167835980653763, 0.002569888485595584, 0.006760775577276945, 0.011543498374521732, 0.04801400378346443, 0.08463207632303238, 0.0], [0.012446260079741478, 0.014791255816817284, 0.01131468079984188, 0.021855689585208893, 0.014415920712053776, 0.014406085014343262, 0.01882690005004406, 0.012486569583415985, 0.013285604305565357, 0.013720812276005745, 0.013047079555690289, 0.01713179238140583, 0.014864268712699413, 0.009690907783806324, 0.00972811970859766, 0.01714640110731125, 0.00830890890210867, 0.018728751689195633, 0.01607953943312168, 0.01689215376973152, 0.008976832032203674, 0.01769641786813736, 0.019405994564294815, 0.014070781879127026, 0.5095201134681702, 0.0025390926748514175, 0.0017912737093865871, 0.0022377108689397573, 0.0017287881346419454, 0.001152808079496026, 0.0021221358329057693, 0.003642226103693247, 0.0013912210706621408, 0.003805403830483556, 0.00434608431532979, 0.0031742893625050783, 0.00508820079267025, 0.014428850263357162, 0.003667468437924981, 0.004817781504243612, 0.00416441960260272, 0.00982570555061102, 0.00656896224245429, 0.0061586108058691025, 0.008982595056295395, 0.01566372439265251, 0.028578655794262886, 0.0052860272116959095]]], \"attentionHeadNames\": [\"L0H0\"], \"tokens\": [\"4\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"2\", \"2\", \"5\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"2\", \"2\"]}\n",
              "    )\n",
              "    </script></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import HTML, IFrame\n",
        "\n",
        "# input_tokens = torch.Tensor([START_TOKEN_ID] + [0] * 5 + [1] * 5 + [MID_TOKEN_ID] + 5 * [0] + 5 * [1]).long().to(DEVICE)\n",
        "\n",
        "# list_to_sort = [1] * LIST_LENGTH // 2 + [0] * (LIST_LENGTH - (LIST_LENGTH // 2))\n",
        "list_to_sort = [1] * 20 + [2] * 3\n",
        "# list_to_sort = [0,1,1,0,1,0,2,3,4,5]\n",
        "# list_to_sort = [0,1,2,3,4,5,6,7,8,9]\n",
        "tokens = torch.Tensor([START_TOKEN_ID] + list_to_sort + [MID_TOKEN_ID] + sorted(list_to_sort)).long().to(DEVICE)\n",
        "\n",
        "# tokens = next(train_gen)[0].to(device=DEVICE)[0]\n",
        "print(tokens.shape)\n",
        "\n",
        "# Run the model and cache all activations\n",
        "original_logits, cache = model.run_with_cache(tokens)\n",
        "\n",
        "positive_html = visualize_attention_patterns(\n",
        "    [0],\n",
        "    cache,\n",
        "    tokens,\n",
        "    f\"What\",\n",
        "    temp=1\n",
        ")\n",
        "\n",
        "\n",
        "HTML(positive_html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([48])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(val_loader))[0][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25\n",
            "Preds:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2]],\n",
            "       device='mps:0')\n",
            "Tokens:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2]],\n",
            "       device='mps:0')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = next(iter(val_loader))[0][0].to(device=DEVICE)\n",
        "\n",
        "list_to_sort = [0] * 10 + [2] * 5 + [1] * 8\n",
        "tokens = torch.Tensor([START_TOKEN_ID] + list_to_sort + [MID_TOKEN_ID] + sorted(list_to_sort)).long().to(DEVICE)\n",
        "\n",
        "tokens = tokens[None, :]\n",
        "val_acc = validate(model, tokens)\n",
        "val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n",
              "       device='mps:0')"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n",
              "       device='mps:0')"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(tokens)[0].argmax(-1)[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
              "        True, True, True, True, True, True, True, True, True, True, True])"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model(tokens)[0].argmax(-1)[24:-1].cpu() == tokens[0][25:].cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 48])\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div style='max-width: 1000px;'><h2>What</h2><br/><div id=\"circuits-vis-2fb151cb-1a37\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, AttentionHeads } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-2fb151cb-1a37\",\n",
              "      AttentionHeads,\n",
              "      {\"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49755486845970154, 0.5024451613426208, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21021544933319092, 0.47134822607040405, 0.3184363543987274, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2205674797296524, 0.22091597318649292, 0.1531732976436615, 0.40534326434135437, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22924786806106567, 0.24563483893871307, 0.14083874225616455, 0.20892339944839478, 0.17535518109798431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.060201846063137054, 0.2246815264225006, 0.10231612622737885, 0.25902894139289856, 0.17253340780735016, 0.18123817443847656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05573727563023567, 0.1426900327205658, 0.12457406520843506, 0.20445631444454193, 0.11283506453037262, 0.20708955824375153, 0.1526176631450653, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06873122602701187, 0.14733491837978363, 0.11183442175388336, 0.12261304259300232, 0.1318463385105133, 0.14909902215003967, 0.14663073420524597, 0.12191031128168106, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09469752013683319, 0.17318932712078094, 0.08917975425720215, 0.08984987437725067, 0.111914724111557, 0.10113076865673065, 0.1221277043223381, 0.09528801590204239, 0.1226222887635231, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.029588084667921066, 0.1380045861005783, 0.07519415766000748, 0.13607779145240784, 0.1176183819770813, 0.11882958561182022, 0.117339588701725, 0.07995228469371796, 0.08502063155174255, 0.10237493366003036, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.042774952948093414, 0.11508408933877945, 0.0630187839269638, 0.07862105220556259, 0.07980337738990784, 0.10725860297679901, 0.1261686384677887, 0.08423414826393127, 0.0968976691365242, 0.07468654215335846, 0.13145211338996887, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.043380845338106155, 0.10283831506967545, 0.07064340263605118, 0.14015072584152222, 0.07498780637979507, 0.09130077809095383, 0.109477199614048, 0.07918015867471695, 0.06879100203514099, 0.07280971109867096, 0.10126933455467224, 0.04517074301838875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.014787632040679455, 0.08710797876119614, 0.03921235352754593, 0.1596241146326065, 0.07120728492736816, 0.07365714013576508, 0.11344049870967865, 0.06075667217373848, 0.061266642063856125, 0.05881264805793762, 0.0686168372631073, 0.10062181949615479, 0.09088839590549469, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07069850713014603, 0.056654490530490875, 0.04631493613123894, 0.1097886711359024, 0.056099895387887955, 0.08931590616703033, 0.1048491895198822, 0.056737128645181656, 0.06695874035358429, 0.05553948134183884, 0.09474527090787888, 0.0902085229754448, 0.058310650289058685, 0.043778710067272186, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16989494860172272, 0.07204407453536987, 0.047660209238529205, 0.12400799244642258, 0.06223555654287338, 0.06233277544379234, 0.10922492295503616, 0.03908374160528183, 0.0476580448448658, 0.0441523976624012, 0.09699651598930359, 0.05065905675292015, 0.03499184921383858, 0.02481790818274021, 0.01424003392457962, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01805485039949417, 0.06432624161243439, 0.05041207745671272, 0.0754002183675766, 0.0693601444363594, 0.08482751250267029, 0.09389933198690414, 0.07394782453775406, 0.06760905683040619, 0.08279910683631897, 0.07245436310768127, 0.056166037917137146, 0.06250022351741791, 0.033315904438495636, 0.03966468200087547, 0.05526243895292282, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.027254076674580574, 0.030073275789618492, 0.020527517423033714, 0.044128552079200745, 0.030354255810379982, 0.037669502198696136, 0.0473773293197155, 0.03219936788082123, 0.025467360392212868, 0.02799409069120884, 0.056677769869565964, 0.1761746108531952, 0.11558419466018677, 0.07503613829612732, 0.06873128563165665, 0.1306287944316864, 0.05412190407514572, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09173949062824249, 0.021113179624080658, 0.020308470353484154, 0.038138438016176224, 0.020592940971255302, 0.03628848120570183, 0.0322362445294857, 0.02905264124274254, 0.021385250613093376, 0.021671779453754425, 0.031178904697299004, 0.08863590657711029, 0.07383609563112259, 0.06366147845983505, 0.05794450268149376, 0.06833789497613907, 0.09873383492231369, 0.18514449894428253, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01462017185986042, 0.030642110854387283, 0.02365025505423546, 0.038348399102687836, 0.02734995260834694, 0.028965463861823082, 0.028372546657919884, 0.023287950083613396, 0.02326207421720028, 0.02245679497718811, 0.03276963531970978, 0.052842576056718826, 0.06555524468421936, 0.05414094403386116, 0.05312877893447876, 0.05501919984817505, 0.11790025234222412, 0.13835808634757996, 0.16932953894138336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.008562615141272545, 0.03464466705918312, 0.025514230132102966, 0.07555433362722397, 0.031028779223561287, 0.048501141369342804, 0.05072004720568657, 0.034107506275177, 0.03197406604886055, 0.03683493658900261, 0.02082986570894718, 0.04703996703028679, 0.05302496254444122, 0.022588009014725685, 0.03390277549624443, 0.02775985561311245, 0.05254795402288437, 0.1697433441877365, 0.1153012290596962, 0.07981972396373749, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03402601554989815, 0.022233104333281517, 0.0158326905220747, 0.038361623883247375, 0.024746546521782875, 0.024400804191827774, 0.04635775461792946, 0.01758432760834694, 0.02247673273086548, 0.02107960171997547, 0.025122156366705894, 0.08745520561933517, 0.07443547248840332, 0.04819570481777191, 0.05323828011751175, 0.10359354317188263, 0.04214257001876831, 0.09321288019418716, 0.06487006694078445, 0.08850408345460892, 0.052130818367004395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006217163987457752, 0.022302526980638504, 0.02533763460814953, 0.02217891253530979, 0.023393677547574043, 0.0268387533724308, 0.024645477533340454, 0.023881005123257637, 0.028363462537527084, 0.02600538358092308, 0.020238075405359268, 0.028089886531233788, 0.0300743468105793, 0.02428082935512066, 0.030420517548918724, 0.03231209143996239, 0.11875994503498077, 0.10799967497587204, 0.09535358846187592, 0.10060137510299683, 0.08645423501729965, 0.09625137597322464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.011015941388905048, 0.03672952204942703, 0.020691242069005966, 0.01857866160571575, 0.026607580482959747, 0.036957211792469025, 0.027579480782151222, 0.0292192455381155, 0.033789750188589096, 0.023343728855252266, 0.032006267458200455, 0.04874872416257858, 0.051142241805791855, 0.04096108302474022, 0.03800685331225395, 0.053031135350465775, 0.06067311018705368, 0.03067973628640175, 0.0704461857676506, 0.09924892336130142, 0.0440996028482914, 0.0957799106836319, 0.0706639289855957, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.018459927290678024, 0.016053030267357826, 0.01525583304464817, 0.02730146236717701, 0.015218384563922882, 0.022524161264300346, 0.024090643972158432, 0.01513430941849947, 0.018759720027446747, 0.015115252695977688, 0.023573990911245346, 0.06180712580680847, 0.05844549834728241, 0.041374728083610535, 0.03777743875980377, 0.057861797511577606, 0.04408230632543564, 0.09241029620170593, 0.07623740285634995, 0.054116249084472656, 0.049150802195072174, 0.07650844007730484, 0.0729035958647728, 0.06583762168884277, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.564488724805415e-05, 0.0785970538854599, 0.09472468495368958, 0.09738858044147491, 0.08074688166379929, 0.07608296722173691, 0.09622971713542938, 0.08159886300563812, 0.08609338104724884, 0.07957479357719421, 0.083002008497715, 0.0010523478267714381, 0.0013396447757259011, 0.0007780775194987655, 0.0008420849917456508, 0.0012494289549067616, 0.012974651530385017, 0.016948727890849113, 0.01621200144290924, 0.017248637974262238, 0.018125690519809723, 0.014647441916167736, 0.015389321371912956, 0.015423058532178402, 0.013694300316274166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08758466690778732, 0.06883326172828674, 0.07709326595067978, 0.07019100338220596, 0.0732828676700592, 0.08575976639986038, 0.09003310650587082, 0.06290005147457123, 0.07608089596033096, 0.06912898272275925, 0.06869003921747208, 0.0031016631983220577, 0.004671486094594002, 0.0036378877703100443, 0.0038824845105409622, 0.003947726916521788, 0.013317358680069447, 0.015572747215628624, 0.012760737910866737, 0.011969457380473614, 0.013074065558612347, 0.011228665709495544, 0.010881273075938225, 0.012604495510458946, 0.03118453547358513, 0.018587501719594002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0016916599124670029, 0.0723177045583725, 0.06402819603681564, 0.06835853308439255, 0.06663993746042252, 0.0597204715013504, 0.07276701182126999, 0.06285079568624496, 0.06047717481851578, 0.06316415220499039, 0.06317500025033951, 0.0073678502812981606, 0.009055630303919315, 0.007245889399200678, 0.007870872505009174, 0.01023151632398367, 0.02445838414132595, 0.031522151082754135, 0.025399040430784225, 0.02786853350698948, 0.028336187824606895, 0.024706056341528893, 0.025159494951367378, 0.030334174633026123, 0.06230292469263077, 0.007835308089852333, 0.015115434303879738, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004092761315405369, 0.06065154820680618, 0.056371770799160004, 0.057196203619241714, 0.058640576899051666, 0.0544966384768486, 0.0588756687939167, 0.05829708278179169, 0.05598319694399834, 0.058339446783065796, 0.057528622448444366, 0.011469746939837933, 0.012796683236956596, 0.010842399671673775, 0.011204815469682217, 0.014203413389623165, 0.03042449802160263, 0.03393057733774185, 0.02994045987725258, 0.032402195036411285, 0.033064574003219604, 0.029453592374920845, 0.029612548649311066, 0.03528691455721855, 0.07760322093963623, 0.006999824661761522, 0.012079456821084023, 0.008211496286094189, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004845855291932821, 0.053359899669885635, 0.051307111978530884, 0.05221223458647728, 0.054047513753175735, 0.05064825341105461, 0.0525171160697937, 0.0527215339243412, 0.05170392617583275, 0.05324593558907509, 0.05208458751440048, 0.014512060210108757, 0.015122968703508377, 0.013771751895546913, 0.014254293404519558, 0.017081163823604584, 0.03383457660675049, 0.0353187657892704, 0.03161676228046417, 0.0340777151286602, 0.03481514751911163, 0.032983727753162384, 0.03193926438689232, 0.036857571452856064, 0.0956873819231987, 0.005036873277276754, 0.009680671617388725, 0.006648560985922813, 0.00806677807122469, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.005050932057201862, 0.04847552254796028, 0.047851722687482834, 0.04877869412302971, 0.04885457083582878, 0.047356873750686646, 0.046684764325618744, 0.04789470136165619, 0.04634600877761841, 0.04836411774158478, 0.0476384311914444, 0.016514630988240242, 0.016966644674539566, 0.016196241602301598, 0.016407353803515434, 0.01947358436882496, 0.034797728061676025, 0.037208735942840576, 0.03429238498210907, 0.03460274636745453, 0.03769434243440628, 0.03561989963054657, 0.033860836178064346, 0.03705259785056114, 0.11542607098817825, 0.004167660605162382, 0.007406187709420919, 0.005384374409914017, 0.00653814198449254, 0.0070935566909611225, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006624450441449881, 0.04247034713625908, 0.04038439691066742, 0.04321175068616867, 0.04131925851106644, 0.04133724793791771, 0.04056334123015404, 0.041303135454654694, 0.04054894298315048, 0.04174904525279999, 0.04201264679431915, 0.017987005412578583, 0.019388830289244652, 0.017654510214924812, 0.018184294924139977, 0.0206705741584301, 0.038393378257751465, 0.041418079286813736, 0.04004930332303047, 0.04154368117451668, 0.041685957461595535, 0.038578107953071594, 0.03752720355987549, 0.04215777665376663, 0.12811167538166046, 0.003957729786634445, 0.007023284677416086, 0.006329596973955631, 0.006406685803085566, 0.005984561052173376, 0.005423194728791714, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006869841832667589, 0.03820858523249626, 0.037186089903116226, 0.039279062300920486, 0.038152821362018585, 0.03772580251097679, 0.03799550235271454, 0.03862396255135536, 0.03752555325627327, 0.037865132093429565, 0.03903547301888466, 0.018928730860352516, 0.02023332193493843, 0.018694275990128517, 0.019570842385292053, 0.021366074681282043, 0.039792243391275406, 0.04447399079799652, 0.0409177802503109, 0.04134819284081459, 0.043724022805690765, 0.04116583988070488, 0.03978389874100685, 0.04228287935256958, 0.1352810263633728, 0.004053102340549231, 0.0061192079447209835, 0.006284635979682207, 0.006738634780049324, 0.006895195227116346, 0.005793255753815174, 0.00808507576584816, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007831691764295101, 0.036180272698402405, 0.034079793840646744, 0.036706116050481796, 0.034516461193561554, 0.033682841807603836, 0.035273220390081406, 0.03468775376677513, 0.03497311845421791, 0.0349930077791214, 0.036366481333971024, 0.020834537222981453, 0.02229877933859825, 0.02020866423845291, 0.02039257436990738, 0.023721523582935333, 0.04292173311114311, 0.04690143093466759, 0.04209437221288681, 0.04376766458153725, 0.045938920229673386, 0.04345245659351349, 0.041992396116256714, 0.044821448624134064, 0.12212444841861725, 0.004281744826585054, 0.006320234853774309, 0.007032438647001982, 0.007400458678603172, 0.006611882708966732, 0.007248591631650925, 0.00865801703184843, 0.011684966273605824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.008911055512726307, 0.031844139099121094, 0.03083130158483982, 0.032512445002794266, 0.03139900416135788, 0.030851224437355995, 0.03165174275636673, 0.03080657683312893, 0.03077608160674572, 0.031205110251903534, 0.032498549669981, 0.022206423804163933, 0.022929321974515915, 0.021919986233115196, 0.02197115495800972, 0.024115655571222305, 0.04730826988816261, 0.050873491913080215, 0.04642973840236664, 0.0481148436665535, 0.05069738253951073, 0.04767468571662903, 0.04619944468140602, 0.04901500418782234, 0.12245591729879379, 0.004448341205716133, 0.006000248249620199, 0.005973365157842636, 0.0057449438609182835, 0.006173006258904934, 0.0053617204539477825, 0.0062438929453492165, 0.007985904812812805, 0.0068700723350048065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009255641140043736, 0.02951166033744812, 0.027838775888085365, 0.029360098764300346, 0.028661010786890984, 0.02734585851430893, 0.02807660400867462, 0.02847774140536785, 0.027219684794545174, 0.028304077684879303, 0.029907088726758957, 0.022640587761998177, 0.024320969358086586, 0.022474871948361397, 0.023392802104353905, 0.025620127096772194, 0.04877414181828499, 0.052379216998815536, 0.04974978044629097, 0.05146486312150955, 0.05247059091925621, 0.050465915352106094, 0.048826027661561966, 0.05321221053600311, 0.12291800230741501, 0.004494131542742252, 0.005120244808495045, 0.005872825160622597, 0.005344910081475973, 0.005799461156129837, 0.004577841144055128, 0.005193009041249752, 0.0072157359682023525, 0.006761705037206411, 0.006951764691621065, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007514233235269785, 0.021362874656915665, 0.019663654267787933, 0.022367330268025398, 0.020536450669169426, 0.020716432482004166, 0.020701896399259567, 0.02018088288605213, 0.02020210213959217, 0.020307324826717377, 0.021562451496720314, 0.017634622752666473, 0.01863330602645874, 0.018002435564994812, 0.01786101423203945, 0.0190780907869339, 0.0655784159898758, 0.06596126407384872, 0.06571929156780243, 0.06682915985584259, 0.07307358831167221, 0.0647100880742073, 0.06427985429763794, 0.06418749690055847, 0.07857923209667206, 0.004443748388439417, 0.0038354131393134594, 0.005415122024714947, 0.006104952655732632, 0.006730974651873112, 0.005745339207351208, 0.006870012730360031, 0.007217517122626305, 0.007840162143111229, 0.0068848407827317715, 0.023668408393859863, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009516580030322075, 0.017524996772408485, 0.01654430665075779, 0.0186240766197443, 0.01715611293911934, 0.01649571768939495, 0.016980458050966263, 0.016605421900749207, 0.016921326518058777, 0.017230259254574776, 0.01720944233238697, 0.017083536833524704, 0.017613718286156654, 0.01727128215134144, 0.0169548150151968, 0.018243666738271713, 0.06767940521240234, 0.06684909760951996, 0.06862466782331467, 0.07084154337644577, 0.07615163177251816, 0.06764993816614151, 0.06627804040908813, 0.06935640424489975, 0.0698896124958992, 0.00401114858686924, 0.0031883663032203913, 0.005001187324523926, 0.005826828069984913, 0.005930091720074415, 0.005614605266600847, 0.005909995641559362, 0.0066804406233131886, 0.0075990259647369385, 0.005403121467679739, 0.02478700503706932, 0.028752172365784645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010031484067440033, 0.014613534323871136, 0.012819048017263412, 0.014798787422478199, 0.014234061352908611, 0.013574191369116306, 0.014384109526872635, 0.014011909253895283, 0.014009793289005756, 0.01385805755853653, 0.014362064190208912, 0.017256714403629303, 0.01714613102376461, 0.01707613468170166, 0.01620876044034958, 0.01819072663784027, 0.0678068995475769, 0.07060693204402924, 0.06922262161970139, 0.07370878756046295, 0.07824268192052841, 0.0709371492266655, 0.06848133355379105, 0.06947412341833115, 0.0671662986278534, 0.0036754910834133625, 0.0025959680788218975, 0.004364552441984415, 0.004568682983517647, 0.005100584123283625, 0.005136256106197834, 0.004403355531394482, 0.005848200526088476, 0.005984311457723379, 0.004233017098158598, 0.023739874362945557, 0.02606973424553871, 0.032057616859674454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01005371194332838, 0.012094089761376381, 0.011190281249582767, 0.012965289875864983, 0.011663448996841908, 0.011168049648404121, 0.011685101315379143, 0.011842138133943081, 0.01134706474840641, 0.011623844504356384, 0.012487043626606464, 0.01719680428504944, 0.017701908946037292, 0.016816522926092148, 0.016132548451423645, 0.018781233578920364, 0.07136969268321991, 0.06962267309427261, 0.07011168450117111, 0.07365595549345016, 0.07591613382101059, 0.06913352012634277, 0.06736582517623901, 0.07137410342693329, 0.058494098484516144, 0.003304299432784319, 0.002112611662596464, 0.0037499095778912306, 0.0039982483722269535, 0.004330134484916925, 0.0047678411938250065, 0.003921752795577049, 0.004843652248382568, 0.005310073960572481, 0.004412256181240082, 0.028734631836414337, 0.030496813356876373, 0.03666750714182854, 0.021557549014687538, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00837082788348198, 0.010174494236707687, 0.009341690689325333, 0.010903362184762955, 0.010226480662822723, 0.00958478543907404, 0.010012219659984112, 0.009744198992848396, 0.009939218871295452, 0.009530364535748959, 0.010208971798419952, 0.017165085300803185, 0.01726655103266239, 0.017240416258573532, 0.01677302084863186, 0.018810784444212914, 0.06719296425580978, 0.07065293937921524, 0.07071321457624435, 0.07060340791940689, 0.07328525930643082, 0.06809145957231522, 0.06715503334999084, 0.0691613107919693, 0.056277286261320114, 0.003364067291840911, 0.0019911974668502808, 0.003726046998053789, 0.0032482955139130354, 0.003953666891902685, 0.004236979875713587, 0.0032929691951721907, 0.004120204597711563, 0.004854501225054264, 0.004237703047692776, 0.028475195169448853, 0.031829457730054855, 0.03312240540981293, 0.021248487755656242, 0.03987346962094307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.011899416334927082, 0.007921085692942142, 0.0071372827515006065, 0.008771279826760292, 0.007904406636953354, 0.0073979878798127174, 0.007830727845430374, 0.007711078505963087, 0.00749647431075573, 0.007535669486969709, 0.008472271263599396, 0.019101327285170555, 0.019360648468136787, 0.01899297721683979, 0.019182946532964706, 0.020827602595090866, 0.06642087548971176, 0.06689315289258957, 0.06666172295808792, 0.06621227413415909, 0.07058896869421005, 0.06408829241991043, 0.06332458555698395, 0.06651674211025238, 0.04526190832257271, 0.0024130730889737606, 0.0015485864132642746, 0.00309390714392066, 0.002715014386922121, 0.0031002433970570564, 0.003171175019815564, 0.002652709372341633, 0.0035799858160316944, 0.004124096129089594, 0.002640299964696169, 0.030706113204360008, 0.030279982835054398, 0.030276227742433548, 0.02319541946053505, 0.03597811982035637, 0.05701331049203873, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.012372273951768875, 0.007609135005623102, 0.006854977924376726, 0.008621896617114544, 0.007584382779896259, 0.007389393635094166, 0.007536706980317831, 0.007261835504323244, 0.007177781779319048, 0.007346500176936388, 0.00811176560819149, 0.029509183019399643, 0.02865096740424633, 0.02821667306125164, 0.0274245236068964, 0.03126541152596474, 0.06069466471672058, 0.0638044998049736, 0.06090807914733887, 0.061266567558050156, 0.06169917806982994, 0.058740027248859406, 0.05785224586725235, 0.06061737611889839, 0.0490841381251812, 0.0027470276691019535, 0.0016789088258519769, 0.003357581328600645, 0.0024012532085180283, 0.0028446612413972616, 0.0034825531765818596, 0.002972658257931471, 0.0033663883805274963, 0.003701071487739682, 0.003368509467691183, 0.02872374653816223, 0.027113495394587517, 0.029698241502046585, 0.017620181664824486, 0.026590244844555855, 0.038385894149541855, 0.03434743732213974, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015735842287540436, 0.007266044616699219, 0.006585877854377031, 0.008114942349493504, 0.006919129751622677, 0.006660128012299538, 0.0065728407353162766, 0.007120057009160519, 0.007000557146966457, 0.006746093276888132, 0.007532987277954817, 0.042059119790792465, 0.039802856743335724, 0.04211874678730965, 0.041585177183151245, 0.04572976380586624, 0.05619702860713005, 0.05937850847840309, 0.05541963875293732, 0.05098416656255722, 0.055911026895046234, 0.05466139689087868, 0.05279574170708656, 0.053102586418390274, 0.04803285002708435, 0.0030931795481592417, 0.0018155486322939396, 0.003466406837105751, 0.002067053457722068, 0.0032141918782144785, 0.0037081120535731316, 0.002542070811614394, 0.0031198221258819103, 0.004278002772480249, 0.003670954145491123, 0.022843830287456512, 0.025536464527249336, 0.02435324341058731, 0.014980901964008808, 0.02429332584142685, 0.02953101322054863, 0.019008804112672806, 0.02444402128458023, 0.0, 0.0, 0.0, 0.0, 0.0], [0.031909044831991196, 0.007721203379333019, 0.00617075152695179, 0.008734967559576035, 0.007062878925353289, 0.006434232462197542, 0.007009682711213827, 0.006828815210610628, 0.006138802506029606, 0.006441114470362663, 0.008891900070011616, 0.018578344956040382, 0.017029082402586937, 0.015107625164091587, 0.015713931992650032, 0.018282128497958183, 0.010306780226528645, 0.01540816854685545, 0.011535617522895336, 0.01078526210039854, 0.009924476034939289, 0.010542870499193668, 0.012572734616696835, 0.010495219379663467, 0.673665463924408, 0.0025196995120495558, 0.0015452364459633827, 0.0036139199510216713, 0.0015979964518919587, 0.001250328030437231, 0.0023685279302299023, 0.0008680121973156929, 0.0009385374141857028, 0.0021411431953310966, 0.0016355111729353666, 0.0029568998143076897, 0.0019901017658412457, 0.004594989120960236, 0.001421152614057064, 0.0015395345399156213, 0.0014048046432435513, 0.004314004443585873, 0.0025842611212283373, 0.007424229755997658, 0.0, 0.0, 0.0, 0.0], [0.05060800909996033, 0.0072652120143175125, 0.00545496866106987, 0.00810280442237854, 0.006417951080948114, 0.005704254377633333, 0.0060292622074484825, 0.006240863353013992, 0.005392767023295164, 0.005689525511115789, 0.008321321569383144, 0.027224035933613777, 0.024962808936834335, 0.023145414888858795, 0.024614473804831505, 0.026485268026590347, 0.009211008436977863, 0.013039038516581059, 0.009938950650393963, 0.008283063769340515, 0.007696202490478754, 0.009407712146639824, 0.011072716675698757, 0.010010963305830956, 0.6153687238693237, 0.0018933144165202975, 0.0013871043920516968, 0.0031877385918051004, 0.0013850515242666006, 0.001212787115946412, 0.0023450858425348997, 0.0009131094557233155, 0.000641902384813875, 0.0023960114922374487, 0.0012290200684219599, 0.0017853697063401341, 0.0015978101873770356, 0.0045629506930708885, 0.0013519222848117352, 0.0011622299207374454, 0.0009259484359063208, 0.002288711490109563, 0.00212095957249403, 0.011666282080113888, 0.02025935985147953, 0.0, 0.0, 0.0], [0.056606169790029526, 0.004883269313722849, 0.0034879734739661217, 0.004720364697277546, 0.00411165039986372, 0.0039177038706839085, 0.0037935308646410704, 0.003882184624671936, 0.003509369445964694, 0.004005533177405596, 0.005645939148962498, 0.030812134966254234, 0.0335952453315258, 0.0294517632573843, 0.028393827378749847, 0.03497736155986786, 0.008440393954515457, 0.009041504003107548, 0.008470790460705757, 0.008166329003870487, 0.006101397797465324, 0.007640790659934282, 0.00953841395676136, 0.010352200828492641, 0.575671374797821, 0.002585527952760458, 0.0017057365039363503, 0.004854184575378895, 0.001215793308801949, 0.0011726301163434982, 0.0018711548764258623, 0.0006768233142793179, 0.0005292335408739746, 0.0016638016095384955, 0.0010581451933830976, 0.0023801401257514954, 0.001948135788552463, 0.004068782087415457, 0.0010227150050923228, 0.0010408756788820028, 0.000754870823584497, 0.0015835626982152462, 0.001621717237867415, 0.016957426443696022, 0.02413005754351616, 0.027941562235355377, 0.0, 0.0], [0.1056474968791008, 0.005618673749268055, 0.003898225026205182, 0.004592051263898611, 0.0040483493357896805, 0.004988756962120533, 0.003517815377563238, 0.004064599052071571, 0.004897713661193848, 0.003946281038224697, 0.0044881729409098625, 0.04701272025704384, 0.03827233240008354, 0.04483427107334137, 0.03354085609316826, 0.030001461505889893, 0.012297295965254307, 0.011337651871144772, 0.01184020098298788, 0.00801078975200653, 0.009875643998384476, 0.010995883494615555, 0.010502586141228676, 0.007753761950880289, 0.3238780200481415, 0.0029292686376720667, 0.0018679046770557761, 0.005037123337388039, 0.0014074011705815792, 0.0014735497534275055, 0.0024499185383319855, 0.0010795376729220152, 0.0008441979298368096, 0.002850535325706005, 0.0012344031129032373, 0.002987064654007554, 0.001975603401660919, 0.002833796665072441, 0.0023610007483512163, 0.0032038441859185696, 0.002515228698030114, 0.003453843528404832, 0.0023880843073129654, 0.03235100209712982, 0.05163292586803436, 0.04461730271577835, 0.07864486426115036, 0.0], [0.01067096833139658, 0.02334040403366089, 0.01840542070567608, 0.03275495767593384, 0.023342004045844078, 0.0236439760774374, 0.029192788526415825, 0.02056499570608139, 0.021468475461006165, 0.021368984133005142, 0.021264303475618362, 0.014128251932561398, 0.012468941509723663, 0.007749170530587435, 0.008433718234300613, 0.013209204189479351, 0.007123754359781742, 0.016057346016168594, 0.013786009512841702, 0.014482714235782623, 0.007696406915783882, 0.016486212611198425, 0.0174171831458807, 0.013116927817463875, 0.4368439018726349, 0.004236333537846804, 0.005554511211812496, 0.006767414975911379, 0.005332353059202433, 0.00348208867944777, 0.004383967723697424, 0.006908324547111988, 0.003186519257724285, 0.006067362148314714, 0.0063928584568202496, 0.002721519907936454, 0.004362437408417463, 0.012370768934488297, 0.0031443533953279257, 0.0041305897757411, 0.00357042090035975, 0.008424201048910618, 0.005631988402456045, 0.0063819303177297115, 0.009469235315918922, 0.013429504819214344, 0.02450229600071907, 0.004532046150416136]]], \"attentionHeadNames\": [\"L0H0\"], \"tokens\": [\"4\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"2\", \"2\", \"2\", \"2\", \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"5\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"2\", \"2\", \"2\", \"2\", \"2\"]}\n",
              "    )\n",
              "    </script></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import HTML, IFrame\n",
        "\n",
        "# input_tokens = torch.Tensor([START_TOKEN_ID] + [0] * 5 + [1] * 5 + [MID_TOKEN_ID] + 5 * [0] + 5 * [1]).long().to(DEVICE)\n",
        "\n",
        "# list_to_sort = [1] * LIST_LENGTH // 2 + [0] * (LIST_LENGTH - (LIST_LENGTH // 2))\n",
        "# list_to_sort = [0] * 17 + [1] * 6\n",
        "# list_to_sort = [0,1,1,0,1,0,2,3,4,5]\n",
        "# list_to_sort = [0,1,2,3,4,5,6,7,8,9]\n",
        "# tokens = torch.Tensor([START_TOKEN_ID] + list_to_sort + [MID_TOKEN_ID] + sorted(list_to_sort)).long().to(DEVICE)\n",
        "\n",
        "# tokens = next(train_gen)[0].to(device=DEVICE)[0]\n",
        "print(tokens.shape)\n",
        "\n",
        "# Run the model and cache all activations\n",
        "original_logits, cache = model.run_with_cache(tokens[0])\n",
        "\n",
        "positive_html = visualize_attention_patterns(\n",
        "    [0],\n",
        "    cache,\n",
        "    tokens[0],\n",
        "    f\"What\",\n",
        "    temp=1\n",
        ")\n",
        "\n",
        "\n",
        "HTML(positive_html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 48])\n",
            "torch.Size([6, 48])\n",
            "tensor([[4, 1, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2,\n",
            "         5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]],\n",
            "       device='mps:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "attn = model.blocks[0].attn\n",
        "all_token_embeddings = model.input_to_embed(tokens)[0][0]\n",
        "\n",
        "print(tokens.shape)\n",
        "\n",
        "# attn.W_V.shape\n",
        "OV_circuit_1 = (attn.W_V[0] @ attn.W_O[0])\n",
        "OV_circuit_2 = model.unembed.W_U\n",
        "OV_circuit_1.shape\n",
        "\n",
        "out = (all_token_embeddings @ OV_circuit_1) @ OV_circuit_2\n",
        "\n",
        "out = torch.softmax(out, dim=-1)\n",
        "out = sharpen_probabilities(out, 1)\n",
        "\n",
        "out = out.transpose(0,1)\n",
        "\n",
        "print(out.shape)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [20, 10]\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "graph = ax.imshow(out.detach().cpu().numpy(), cmap=\"viridis\", interpolation=\"nearest\")\n",
        "cbar = plt.colorbar(graph, orientation='horizontal', shrink=0.5, pad=0.04)\n",
        "plt.tight_layout()\n",
        "\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([48, 128])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.pos_embed(tokens)[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3]],\n",
            "       device='mps:0')\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3]],\n",
            "       device='mps:0')\n",
            "torch.Size([1, 48])\n",
            "torch.Size([6, 48])\n",
            "tensor([[4, 3, 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 3, 2, 2, 2, 2, 2, 1, 1, 3, 3,\n",
            "         5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3]],\n",
            "       device='mps:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokens = next(iter(val_loader))[0][0].to(device=DEVICE)\n",
        "\n",
        "tokens = tokens[None, :]\n",
        "val_acc = validate(model, tokens)\n",
        "val_acc\n",
        "\n",
        "attn = model.blocks[0].attn\n",
        "all_token_embeddings = model.pos_embed(tokens)[0]\n",
        "\n",
        "print(tokens.shape)\n",
        "\n",
        "# attn.W_V.shape\n",
        "OV_circuit_1 = (attn.W_V[0] @ attn.W_O[0])\n",
        "OV_circuit_2 = model.unembed.W_U\n",
        "OV_circuit_1.shape\n",
        "\n",
        "out = (all_token_embeddings @ OV_circuit_1) @ OV_circuit_2\n",
        "\n",
        "out = torch.softmax(out, dim=-1)\n",
        "out = sharpen_probabilities(out, 1)\n",
        "\n",
        "out = out.transpose(0,1)\n",
        "\n",
        "print(out.shape)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [20, 10]\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "graph = ax.imshow(out.detach().cpu().numpy(), cmap=\"viridis\", interpolation=\"nearest\")\n",
        "cbar = plt.colorbar(graph, orientation='horizontal', shrink=0.5, pad=0.04)\n",
        "plt.tight_layout()\n",
        "\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25\n",
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='mps:0')\n",
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
            "       device='mps:0')\n",
            "1.0\n",
            "torch.Size([48])\n",
            "torch.Size([6, 48])\n",
            "tensor([4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       device='mps:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokens = torch.Tensor([START_TOKEN_ID] + [0] * 23 + [MID_TOKEN_ID] + [0] * 23).long().to(DEVICE)\n",
        "val_acc = validate(model, tokens[None, :])\n",
        "print(val_acc)\n",
        "\n",
        "attn = model.blocks[0].attn\n",
        "all_token_embeddings = model.pos_embed(tokens)[0]\n",
        "# all_token_embeddings = model.embed(tokens)\n",
        "\n",
        "print(tokens.shape)\n",
        "\n",
        "# attn.W_V.shape\n",
        "OV_circuit_1 = (attn.W_V[0] @ attn.W_O[0])\n",
        "OV_circuit_2 = model.unembed.W_U\n",
        "OV_circuit_1.shape\n",
        "\n",
        "out = (all_token_embeddings @ OV_circuit_1) @ OV_circuit_2\n",
        "\n",
        "out = torch.softmax(out, dim=-1)\n",
        "out = sharpen_probabilities(out, 1)\n",
        "\n",
        "out = out.transpose(0,1)\n",
        "\n",
        "print(out.shape)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [20, 10]\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "graph = ax.imshow(out.detach().cpu().numpy(), cmap=\"viridis\", interpolation=\"nearest\")\n",
        "cbar = plt.colorbar(graph, orientation='horizontal', shrink=0.5, pad=0.04)\n",
        "plt.tight_layout()\n",
        "\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       device='mps:0')"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25\n",
            "Preds:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2]],\n",
            "       device='mps:0')\n",
            "Tokens:  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2]],\n",
            "       device='mps:0')\n",
            "1.0\n",
            "tensor([4, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1,\n",
            "        5, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2])\n",
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2]],\n",
            "       device='mps:0')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div id=\"circuits-vis-d6a8ae95-1a09\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-d6a8ae95-1a09\",\n",
              "      AttentionPatterns,\n",
              "      {\"tokens\": [\"tensor(4)\", \"tensor(0)\", \"tensor(1)\", \"tensor(2)\", \"tensor(0)\", \"tensor(1)\", \"tensor(2)\", \"tensor(0)\", \"tensor(1)\", \"tensor(2)\", \"tensor(0)\", \"tensor(1)\", \"tensor(2)\", \"tensor(0)\", \"tensor(1)\", \"tensor(2)\", \"tensor(0)\", \"tensor(1)\", \"tensor(2)\", \"tensor(0)\", \"tensor(1)\", \"tensor(2)\", \"tensor(0)\", \"tensor(1)\", \"tensor(5)\", \"tensor(0)\", \"tensor(0)\", \"tensor(0)\", \"tensor(0)\", \"tensor(0)\", \"tensor(0)\", \"tensor(0)\", \"tensor(0)\", \"tensor(1)\", \"tensor(1)\", \"tensor(1)\", \"tensor(1)\", \"tensor(1)\", \"tensor(1)\", \"tensor(1)\", \"tensor(1)\", \"tensor(2)\", \"tensor(2)\", \"tensor(2)\", \"tensor(2)\", \"tensor(2)\", \"tensor(2)\", \"tensor(2)\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49755486845970154, 0.5024451613426208, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09133174270391464, 0.28847604990005493, 0.6201921701431274, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22364535927772522, 0.2683994472026825, 0.16022221744060516, 0.3477329909801483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13701283931732178, 0.14680670201778412, 0.31366926431655884, 0.29770800471305847, 0.10480320453643799, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015450781211256981, 0.08668740093708038, 0.21378445625305176, 0.195831298828125, 0.06679031252861023, 0.4214557409286499, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.042618170380592346, 0.13815884292125702, 0.1809716522693634, 0.13620568811893463, 0.10658851265907288, 0.3086567521095276, 0.08680044859647751, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04953595995903015, 0.10618720203638077, 0.1689911186695099, 0.11574626713991165, 0.09502425789833069, 0.23357976973056793, 0.14307230710983276, 0.08786317706108093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02360549196600914, 0.060769274830818176, 0.17497774958610535, 0.0886579379439354, 0.039082374423742294, 0.20615404844284058, 0.13085664808750153, 0.03279556334018707, 0.24310088157653809, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.040715765208005905, 0.21842220425605774, 0.07985079288482666, 0.06354749202728271, 0.18468017876148224, 0.12096951901912689, 0.04517442733049393, 0.11937680095434189, 0.08591447025537491, 0.04134834557771683, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03784259781241417, 0.10181380808353424, 0.0714108943939209, 0.0796685740351677, 0.07060129940509796, 0.12718713283538818, 0.13323038816452026, 0.07452115416526794, 0.11258551478385925, 0.07484413683414459, 0.11629442870616913, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.012598542496562004, 0.03483505919575691, 0.15215924382209778, 0.07752074301242828, 0.02581726387143135, 0.21527868509292603, 0.06764837354421616, 0.029103310778737068, 0.15015628933906555, 0.04495226964354515, 0.03167526051402092, 0.1582549512386322, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015383817255496979, 0.09061984717845917, 0.03734665364027023, 0.15052273869514465, 0.07407809793949127, 0.07095581293106079, 0.10565764456987381, 0.06320615857839584, 0.05870829522609711, 0.05601342022418976, 0.07138321548700333, 0.11157168447971344, 0.09455268830060959, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.032027993351221085, 0.021026408299803734, 0.06574372947216034, 0.1175348311662674, 0.02110148034989834, 0.14093993604183197, 0.13639067113399506, 0.022778131067752838, 0.10010065138339996, 0.06873782724142075, 0.033580340445041656, 0.13976268470287323, 0.07889457046985626, 0.021380774676799774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07632984966039658, 0.033654309809207916, 0.08331703394651413, 0.10292062163352966, 0.029815031215548515, 0.11760395765304565, 0.10276907682418823, 0.01926686242222786, 0.08464883267879486, 0.04037192463874817, 0.042721226811409, 0.1496603637933731, 0.05203530192375183, 0.024228353053331375, 0.040657274425029755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.017232827842235565, 0.06139751896262169, 0.05672077089548111, 0.049420423805713654, 0.06620222330093384, 0.09800753742456436, 0.06055362522602081, 0.07058104872703552, 0.07656392455101013, 0.05349068343639374, 0.06915557384490967, 0.09503952413797379, 0.059654634445905685, 0.046811651438474655, 0.06642168015241623, 0.05274638533592224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.028490940108895302, 0.02373354509472847, 0.03524551913142204, 0.12395341694355011, 0.02348669432103634, 0.06546661257743835, 0.1489199846982956, 0.024964045733213425, 0.042960118502378464, 0.08400866389274597, 0.044438689947128296, 0.08477307856082916, 0.10018578916788101, 0.015756791457533836, 0.03238512575626373, 0.10863757133483887, 0.012593451887369156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05383031442761421, 0.012388656847178936, 0.08285307884216309, 0.07311791926622391, 0.012083394452929497, 0.15619619190692902, 0.06659138947725296, 0.017047323286533356, 0.0841723084449768, 0.043952576816082, 0.018294956535100937, 0.10088938474655151, 0.043325070291757584, 0.01062693353742361, 0.06623434275388718, 0.040098875761032104, 0.009659345261752605, 0.10863789916038513, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03808726742863655, 0.07119120657444, 0.04613526165485382, 0.058322932571172714, 0.061477791517972946, 0.05415893346071243, 0.03780761733651161, 0.04935896396636963, 0.042853135615587234, 0.030383411794900894, 0.08218617737293243, 0.05684195086359978, 0.04761549457907677, 0.05148860067129135, 0.050238922238349915, 0.036594733595848083, 0.05326874554157257, 0.0828758180141449, 0.04911299794912338, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.012674091383814812, 0.03572763502597809, 0.046628307551145554, 0.1044536828994751, 0.03160851448774338, 0.08851872384548187, 0.07487303018569946, 0.03469129279255867, 0.05744065344333649, 0.053455986082553864, 0.022619273513555527, 0.06649082154035568, 0.06166064739227295, 0.01784334145486355, 0.04859223961830139, 0.03242846578359604, 0.019457925111055374, 0.10627423226833344, 0.056794583797454834, 0.027766535058617592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.027696238830685616, 0.018097136169672012, 0.04248065873980522, 0.07894385606050491, 0.02014300972223282, 0.06782139092683792, 0.10382923483848572, 0.014313158579170704, 0.06000453978776932, 0.044658709317445755, 0.020448746159672737, 0.09063761681318283, 0.060588426887989044, 0.014824235811829567, 0.05456254258751869, 0.08432229608297348, 0.01140090823173523, 0.07587272673845291, 0.04410975053906441, 0.022811824455857277, 0.04243304952979088, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01832299306988716, 0.05930693820118904, 0.05879886448383331, 0.025125907734036446, 0.060115840286016464, 0.06019659340381622, 0.024007083848118782, 0.05778544396162033, 0.06323063373565674, 0.025327233597636223, 0.05807260796427727, 0.05789022520184517, 0.02587609551846981, 0.043054621666669846, 0.05637694150209427, 0.02543104812502861, 0.06220880150794983, 0.07768560200929642, 0.020554814487695694, 0.057687290012836456, 0.04273148998618126, 0.020212922245264053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01630755513906479, 0.040872883051633835, 0.029886415228247643, 0.03565260022878647, 0.029002675786614418, 0.05579535290598869, 0.05673031136393547, 0.032002586871385574, 0.050045762211084366, 0.0458059161901474, 0.03633250668644905, 0.046472739428281784, 0.066677987575531, 0.0276749636977911, 0.03580470755696297, 0.06661195307970047, 0.031176090240478516, 0.022615298628807068, 0.067538782954216, 0.048150066286325455, 0.029413294047117233, 0.09318989515304565, 0.03623960539698601, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015690330415964127, 0.013644546270370483, 0.057637348771095276, 0.06718552857637405, 0.012935125268995762, 0.0883207842707634, 0.06363871693611145, 0.012863664887845516, 0.06963937729597092, 0.03904221951961517, 0.02003711648285389, 0.07526019960641861, 0.04967675358057022, 0.011405643075704575, 0.04549345374107361, 0.049180615693330765, 0.009505403228104115, 0.0785457044839859, 0.04716644808650017, 0.011502871289849281, 0.041776563972234726, 0.04840729013085365, 0.015484508126974106, 0.05595981329679489, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [4.651245035347529e-05, 0.10256005078554153, 0.021967245265841484, 0.0021661820355802774, 0.1053653284907341, 0.018723636865615845, 0.001772390678524971, 0.10647705942392349, 0.021802855655550957, 0.001315556699410081, 0.10830800980329514, 0.019932948052883148, 0.001748081180267036, 0.08223318308591843, 0.01699812151491642, 0.0016303599113598466, 0.08401381969451904, 0.022116124629974365, 0.0013746857875958085, 0.10519258677959442, 0.02365192584693432, 0.001337557565420866, 0.11127101629972458, 0.020125307142734528, 0.017869476228952408, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10480295866727829, 0.08236519247293472, 0.01275404542684555, 0.005097881890833378, 0.08768955618143082, 0.014471655711531639, 0.005656897090375423, 0.07526557892560959, 0.01331330370157957, 0.004281401168555021, 0.08219382166862488, 0.010341277346014977, 0.005589854903519154, 0.08694592118263245, 0.013320565223693848, 0.0047238110564649105, 0.09919334948062897, 0.018634194508194923, 0.005027970299124718, 0.08903275430202484, 0.015644297003746033, 0.004494403954595327, 0.0845201313495636, 0.015082414261996746, 0.03731510788202286, 0.0222416203469038, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.001927028875797987, 0.08237962424755096, 0.027820538729429245, 0.010696186684072018, 0.07591187953948975, 0.02674303762614727, 0.010439076460897923, 0.07159553468227386, 0.02746746316552162, 0.00851727556437254, 0.07196485251188278, 0.02756417728960514, 0.010315584018826485, 0.07064476609230042, 0.03067784197628498, 0.011655077338218689, 0.06777992099523544, 0.03590798005461693, 0.008580551482737064, 0.07494021207094193, 0.032278742641210556, 0.00862846989184618, 0.07389400899410248, 0.03455471619963646, 0.07097143679857254, 0.008925472386181355, 0.017218519002199173, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004510227590799332, 0.06683807075023651, 0.03138654679059982, 0.01410247478634119, 0.06462198495864868, 0.031143533065915108, 0.01358721125870943, 0.06424345076084137, 0.03230792656540871, 0.012820779345929623, 0.06339661031961441, 0.03246798366308212, 0.014101958833634853, 0.060381483286619186, 0.0326237827539444, 0.01565217785537243, 0.06271868199110031, 0.0373915359377861, 0.012638314627110958, 0.06525338441133499, 0.036437198519706726, 0.012778166681528091, 0.06411700695753098, 0.03888621926307678, 0.08551883697509766, 0.007713814731687307, 0.013311575166881084, 0.009049078449606895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.005236093886196613, 0.057656992226839066, 0.03360985592007637, 0.01693873666226864, 0.058399979025125504, 0.0340169258415699, 0.016177652403712273, 0.05696721747517586, 0.03491847217082977, 0.01568463444709778, 0.056278977543115616, 0.03522195294499397, 0.016340825706720352, 0.054866645485162735, 0.03550329804420471, 0.01845671609044075, 0.057713497430086136, 0.03816299885511398, 0.015079370699822903, 0.05703810229897499, 0.03761882334947586, 0.016117414459586143, 0.05697290599346161, 0.03982572630047798, 0.10339312255382538, 0.005442494060844183, 0.010460259392857552, 0.007183972280472517, 0.008716398850083351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.005375849548727274, 0.05159386619925499, 0.035866595804691315, 0.019128870218992233, 0.05199730023741722, 0.03644157201051712, 0.017571477219462395, 0.05097568407654762, 0.03562578931450844, 0.01738774962723255, 0.05070292577147484, 0.036835167557001114, 0.018058080226182938, 0.05119229853153229, 0.03758019208908081, 0.020726285874843597, 0.05095904693007469, 0.039602309465408325, 0.01738392561674118, 0.049758266657590866, 0.04011915624141693, 0.018503079190850258, 0.05176937207579613, 0.03943612799048424, 0.12285122275352478, 0.004435759037733078, 0.007882614620029926, 0.005730742122977972, 0.00695872912183404, 0.00754987308755517, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006932231597602367, 0.044443581253290176, 0.03900167718529701, 0.020881930366158485, 0.043239008635282516, 0.04089844971895218, 0.018965335562825203, 0.04322213679552078, 0.03985854238271713, 0.01887591741979122, 0.04396461322903633, 0.040211379528045654, 0.02028966322541237, 0.04286721348762512, 0.04181092232465744, 0.02163095772266388, 0.04303361847996712, 0.04334241896867752, 0.019536737352609634, 0.045595090836286545, 0.04362274333834648, 0.019303390756249428, 0.043535053730010986, 0.044116485863924026, 0.13406391441822052, 0.0041416110470891, 0.00734959589317441, 0.006623678840696812, 0.006704349536448717, 0.0062626120634377, 0.005675164051353931, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007115150801837444, 0.03957293927669525, 0.040739212185144424, 0.021538976579904556, 0.039515186101198196, 0.042226649820804596, 0.020292337983846664, 0.04000315070152283, 0.0416569821536541, 0.019665604457259178, 0.04042935371398926, 0.04123122617602348, 0.02095581591129303, 0.03869899734854698, 0.043891791254282, 0.022129015997052193, 0.03892907872796059, 0.04606207087635994, 0.02006724290549755, 0.03976844996213913, 0.04528532549738884, 0.020691055804491043, 0.040090836584568024, 0.0437927208840847, 0.14011165499687195, 0.0041978307999670506, 0.006337713450193405, 0.006509048398584127, 0.006979258265346289, 0.007141409441828728, 0.0060001215897500515, 0.008373777382075787, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.008036592043936253, 0.03712685406208038, 0.042633116245269775, 0.023037593811750412, 0.035419512540102005, 0.04284714162349701, 0.02168126590549946, 0.03559529036283493, 0.04413481056690216, 0.02106405794620514, 0.03731793537735939, 0.04424319416284561, 0.022882182151079178, 0.035417474806308746, 0.04438260197639465, 0.02434214949607849, 0.036448944360017776, 0.048128511756658554, 0.020819010213017464, 0.036626435816287994, 0.04714081808924675, 0.022000806406140327, 0.03657186031341553, 0.0459941104054451, 0.12531958520412445, 0.004393768031150103, 0.006485590711236, 0.007216427940875292, 0.0075940764509141445, 0.0067848688922822475, 0.0074382359161973, 0.008884536102414131, 0.011990679427981377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.005172383040189743, 0.025797689333558083, 0.06329143792390823, 0.017108425498008728, 0.025901705026626587, 0.065447598695755, 0.01613129861652851, 0.02527921460568905, 0.06312540173530579, 0.015652971342206, 0.025851856917142868, 0.062182582914829254, 0.015807688236236572, 0.026223063468933105, 0.06441085785627365, 0.017266983166337013, 0.026639988645911217, 0.062387317419052124, 0.015209545381367207, 0.026938725262880325, 0.06833995133638382, 0.01622415892779827, 0.027481256052851677, 0.06278648227453232, 0.09198062866926193, 0.004212956875562668, 0.00422256113961339, 0.0056820823810994625, 0.006692890077829361, 0.007219517137855291, 0.006002489943057299, 0.007062116637825966, 0.00855203252285719, 0.017714185640215874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.005624283105134964, 0.023605063557624817, 0.0640651062130928, 0.01774774305522442, 0.02334875985980034, 0.06478971242904663, 0.01656341180205345, 0.023099735379219055, 0.06224403902888298, 0.01652979664504528, 0.02347944676876068, 0.06085747107863426, 0.016796551644802094, 0.02283615805208683, 0.06601522862911224, 0.018323905766010284, 0.023524608463048935, 0.06163284555077553, 0.016373181715607643, 0.024608571082353592, 0.0677817091345787, 0.017223648726940155, 0.024775227531790733, 0.06548656523227692, 0.08658121526241302, 0.004556756932288408, 0.003825106890872121, 0.005903860088437796, 0.006562780123203993, 0.0071234614588320255, 0.005413929466158152, 0.006164104212075472, 0.008036705665290356, 0.020435892045497894, 0.018063358962535858, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007273312658071518, 0.02067793905735016, 0.06256338208913803, 0.018835658207535744, 0.01987801305949688, 0.0677681639790535, 0.01734006032347679, 0.01953384466469288, 0.06357116997241974, 0.01683386228978634, 0.0208711177110672, 0.06379500776529312, 0.01803588680922985, 0.020358791574835777, 0.06792432069778442, 0.018466411158442497, 0.020867295563220978, 0.0638464167714119, 0.01700012758374214, 0.020815346390008926, 0.07073070853948593, 0.017500802874565125, 0.02100139856338501, 0.06212952360510826, 0.07605983316898346, 0.004301273263990879, 0.0037124422378838062, 0.005241502542048693, 0.0059092165902256966, 0.006515166722238064, 0.005561132915318012, 0.006649747025221586, 0.0069861095398664474, 0.020377587527036667, 0.01815788820385933, 0.022909553721547127, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009105690754950047, 0.016768336296081543, 0.0664268359541893, 0.017977595329284668, 0.01641537807881832, 0.06780961155891418, 0.016391562297940254, 0.01588846556842327, 0.06676163524389267, 0.01656612753868103, 0.016466405242681503, 0.0660228431224823, 0.016853226348757744, 0.0164298415184021, 0.06897323578596115, 0.017455976456403732, 0.016992319375276566, 0.06396281719207764, 0.01623670570552349, 0.017388036474585533, 0.07286370545625687, 0.016777722164988518, 0.017016921192407608, 0.06636185944080353, 0.06687205284833908, 0.003837962867692113, 0.003050705185160041, 0.004785255528986454, 0.005575248505920172, 0.0056740534491837025, 0.0053721885196864605, 0.005654824897646904, 0.0063920048996806145, 0.02424619160592556, 0.017399070784449577, 0.023716799914836884, 0.027510765939950943, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009519590064883232, 0.013867824338376522, 0.0648631751537323, 0.0167218130081892, 0.013507715426385403, 0.07045463472604752, 0.016409585252404213, 0.013296900317072868, 0.06941378861665726, 0.01581604965031147, 0.013629186898469925, 0.07017113268375397, 0.016271185129880905, 0.013457576744258404, 0.06929858773946762, 0.017262477427721024, 0.013509385287761688, 0.06700395047664642, 0.015366358682513237, 0.014331575483083725, 0.07425005733966827, 0.016537578776478767, 0.013891367241740227, 0.06592895090579987, 0.06373888999223709, 0.003487935522571206, 0.0024634990841150284, 0.004141834564507008, 0.0043355487287044525, 0.004840307869017124, 0.004874159581959248, 0.004178658127784729, 0.005549774505198002, 0.023249587044119835, 0.016669705510139465, 0.022528458386659622, 0.024739429354667664, 0.030421758070588112, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009434584528207779, 0.011349312961101532, 0.06846897304058075, 0.017437787726521492, 0.010945191606879234, 0.06980634480714798, 0.015970787033438683, 0.011112876236438751, 0.06735014915466309, 0.01598935015499592, 0.011718067340552807, 0.06884782016277313, 0.016611790284514427, 0.01073049008846283, 0.06791967153549194, 0.01762464828789234, 0.011668229475617409, 0.06533516943454742, 0.01547303982079029, 0.011786999180912971, 0.07124106585979462, 0.016032680869102478, 0.011189303360879421, 0.06697875261306763, 0.0548919178545475, 0.0031008142977952957, 0.0019825128838419914, 0.0035189830232411623, 0.003752028336748481, 0.004063475877046585, 0.004474228248000145, 0.0036802436225116253, 0.0045453705824911594, 0.024266790598630905, 0.020477214828133583, 0.026965098455548286, 0.028618762269616127, 0.03440944850444794, 0.020229993388056755, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007808607537299395, 0.009491132572293282, 0.06518816202878952, 0.017377914860844612, 0.009539627470076084, 0.0682603195309639, 0.01635611429810524, 0.009089737199246883, 0.06711392104625702, 0.015764370560646057, 0.009523293934762478, 0.06460611522197723, 0.016106856986880302, 0.00903975311666727, 0.06647225469350815, 0.017547372728586197, 0.00962255708873272, 0.06590759009122849, 0.01639639213681221, 0.009905902668833733, 0.06836311519145966, 0.016579393297433853, 0.00971299596130848, 0.06451614201068878, 0.052497465163469315, 0.003138122148811817, 0.001857460243627429, 0.003475789912045002, 0.0030301264487206936, 0.003688122145831585, 0.003952406346797943, 0.003071799408644438, 0.0038434742018580437, 0.02488889917731285, 0.022097904235124588, 0.026562679558992386, 0.029691657051444054, 0.03089776448905468, 0.019821349531412125, 0.03719539940357208, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010990866459906101, 0.00731629179790616, 0.061841949820518494, 0.019258776679635048, 0.007300886325538158, 0.06534335762262344, 0.01794266141951084, 0.00712231732904911, 0.06237594783306122, 0.017481649294495583, 0.007825392298400402, 0.06267718970775604, 0.017882416024804115, 0.006931549869477749, 0.06629939377307892, 0.019237365573644638, 0.007623354904353619, 0.06178569421172142, 0.017547734081745148, 0.007470312993973494, 0.06519932299852371, 0.01767776533961296, 0.007317221723496914, 0.061438027769327164, 0.04180605337023735, 0.002228828612715006, 0.0014303482603281736, 0.0028576801996678114, 0.0025077168829739094, 0.0028635323978960514, 0.00292904837988317, 0.0024501688312739134, 0.0033066454343497753, 0.025536727160215378, 0.016585910692811012, 0.028361625969409943, 0.02796803042292595, 0.02796456590294838, 0.021424390375614166, 0.03323110565543175, 0.052660200744867325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.014378908090293407, 0.008651056326925755, 0.0105124581605196, 0.01040378026664257, 0.008357404731214046, 0.010550316423177719, 0.008269011974334717, 0.00760964909568429, 0.01019090786576271, 0.00853169709444046, 0.009820930659770966, 0.012746315449476242, 0.009396608918905258, 0.00705358199775219, 0.01051352545619011, 0.009322723373770714, 0.007799030747264624, 0.01594114489853382, 0.00849888939410448, 0.008592427708208561, 0.0109162088483572, 0.0078948475420475, 0.009084035642445087, 0.01189096737653017, 0.7217638492584229, 0.0026999267283827066, 0.002090645954012871, 0.003663692157715559, 0.001637033885344863, 0.0011251969262957573, 0.0018726045964285731, 0.0012950041564181447, 0.0011594268726184964, 0.0021552713587880135, 0.002228165278211236, 0.0035183471627533436, 0.0018788358429446816, 0.0033913999795913696, 0.00152633897960186, 0.0021527723874896765, 0.002437029732391238, 0.006477893330156803, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01891709491610527, 0.009537569247186184, 0.009331637062132359, 0.013631184585392475, 0.008896068669855595, 0.008783713914453983, 0.010443729348480701, 0.008599679917097092, 0.009155655279755592, 0.011210370808839798, 0.010540146380662918, 0.01104068011045456, 0.012310479767620564, 0.007976468652486801, 0.009534168057143688, 0.012617134489119053, 0.008692391216754913, 0.014303766191005707, 0.011683178134262562, 0.00885319709777832, 0.009630169719457626, 0.01094945427030325, 0.010014650411903858, 0.010228791274130344, 0.693779706954956, 0.0036900038830935955, 0.002890216652303934, 0.0045981453731656075, 0.0018352369079366326, 0.001535767805762589, 0.0023963581770658493, 0.0014177141711115837, 0.0013369463849812746, 0.0024222969077527523, 0.0023915490601211786, 0.0029903200920671225, 0.001824488048441708, 0.0029395760502666235, 0.00142195716034621, 0.0020529453177005053, 0.001967150019481778, 0.006574153900146484, 0.00505412882193923, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03109562397003174, 0.0075243753381073475, 0.009269842877984047, 0.01893937960267067, 0.0068828328512609005, 0.009285672567784786, 0.015342081896960735, 0.006654736120253801, 0.008856010623276234, 0.015438353642821312, 0.008665229193866253, 0.011497680097818375, 0.016594979912042618, 0.00602682726457715, 0.0093699274584651, 0.017816083505749702, 0.006754275411367416, 0.01501538511365652, 0.01733042486011982, 0.007241636514663696, 0.009671482257544994, 0.015487069264054298, 0.00774686923250556, 0.01022767648100853, 0.6564924716949463, 0.002455467591062188, 0.001505845575593412, 0.0035217944532632828, 0.0015572605188935995, 0.0012184547958895564, 0.00230814958922565, 0.0008458849624730647, 0.0009146123775281012, 0.0028011035174131393, 0.002380989957600832, 0.0028815229889005423, 0.001939370296895504, 0.00447785435244441, 0.001384924864396453, 0.0015002889558672905, 0.0013689935440197587, 0.008062867447733879, 0.006412661634385586, 0.007234972435981035, 0.0, 0.0, 0.0, 0.0], [0.04821692034602165, 0.006921950727701187, 0.007609608583152294, 0.027745964005589485, 0.006114720366895199, 0.0075645167380571365, 0.02130562625825405, 0.005945999640971422, 0.0071777598932385445, 0.022215690463781357, 0.007928161881864071, 0.009394323453307152, 0.023783383890986443, 0.005398280452936888, 0.008080380037426949, 0.025233911350369453, 0.006188822444528341, 0.012422979809343815, 0.02582753635942936, 0.005792934447526932, 0.0073325783014297485, 0.023530272766947746, 0.006908711511641741, 0.009537972509860992, 0.5862942337989807, 0.0018038604175671935, 0.001321567571721971, 0.003037126502022147, 0.0013196116778999567, 0.0011554862139746547, 0.0022342868614941835, 0.0008699675090610981, 0.0006115742726251483, 0.0029502122197300196, 0.0016862292541190982, 0.0017010158626362681, 0.0015223179943859577, 0.0043473634868860245, 0.0012880476424470544, 0.0011073177447542548, 0.0008821998490020633, 0.007855406031012535, 0.00941590778529644, 0.011115082539618015, 0.019302161410450935, 0.0, 0.0, 0.0], [0.05318917706608772, 0.004588494077324867, 0.006200037896633148, 0.02809721976518631, 0.0038634533993899822, 0.006562946364283562, 0.023869799450039864, 0.0036478391848504543, 0.005865348968654871, 0.02847955748438835, 0.005305125843733549, 0.007266212720423937, 0.0315672867000103, 0.0036542739253491163, 0.006293402053415775, 0.0328659750521183, 0.004400813486427069, 0.00849571917206049, 0.03130555525422096, 0.004441344644874334, 0.005733090918511152, 0.026880331337451935, 0.004538713488727808, 0.0097272964194417, 0.5409213900566101, 0.0024294541217386723, 0.001602770877070725, 0.004561164882034063, 0.0011424027616158128, 0.0011018450604751706, 0.0017582039581611753, 0.0006359672988764942, 0.0004972867318429053, 0.0025092759169638157, 0.0018122654873877764, 0.002236464526504278, 0.0018305379198864102, 0.003823172301054001, 0.000960979494266212, 0.0009780438849702477, 0.0007093035383149981, 0.007988586090505123, 0.0107997702434659, 0.015933802351355553, 0.022673461586236954, 0.026254888623952866, 0.0, 0.0], [0.09568651765584946, 0.005088916514068842, 0.007369533181190491, 0.03344353288412094, 0.003666650503873825, 0.009011468850076199, 0.027439143508672714, 0.00368136796168983, 0.008828743360936642, 0.03466244414448738, 0.004065005574375391, 0.009222902357578278, 0.03466382250189781, 0.004181350581347942, 0.006067399866878986, 0.027172770351171494, 0.005595738068223, 0.0102686807513237, 0.04971740022301674, 0.0038707349449396133, 0.008944517932832241, 0.044018253684043884, 0.0044163549318909645, 0.0070226979441940784, 0.29334115982055664, 0.0026530823670327663, 0.0016917891334742308, 0.004562197718769312, 0.0012747043510898948, 0.001334615983068943, 0.0022189277224242687, 0.0009777533123269677, 0.0007646026788279414, 0.004619026556611061, 0.0022111909929662943, 0.002705428982153535, 0.0017893335316330194, 0.002566611859947443, 0.002138393698260188, 0.0029017694760113955, 0.0022780802100896835, 0.01767715811729431, 0.016482340171933174, 0.029300784692168236, 0.04676470905542374, 0.04041055589914322, 0.07122982293367386, 0.0], [0.011067763902246952, 0.024208305403590202, 0.01006151270121336, 0.018170833587646484, 0.024209966883063316, 0.01281052641570568, 0.015445353463292122, 0.021329695358872414, 0.011814145371317863, 0.011743340641260147, 0.0220550075173378, 0.015234346501529217, 0.012932593934237957, 0.01532357931137085, 0.008650673553347588, 0.01370038278400898, 0.01365397684276104, 0.01665443181991577, 0.013299291953444481, 0.02727132849395275, 0.007982594892382622, 0.01573643647134304, 0.03190496191382408, 0.01360467541962862, 0.4530877470970154, 0.004393859766423702, 0.005761053413152695, 0.007019058335572481, 0.005530634429305792, 0.0036115688271820545, 0.0045469836331903934, 0.007165207527577877, 0.0033050086349248886, 0.00338393310084939, 0.0038647297769784927, 0.0028227183502167463, 0.0045246523804962635, 0.012830770574510098, 0.0032612746581435204, 0.004284183494746685, 0.0037031853571534157, 0.009654989466071129, 0.007899189367890358, 0.00661923922598362, 0.009821344166994095, 0.01392887532711029, 0.025413401424884796, 0.004700568504631519]]]}\n",
              "    )\n",
              "    </script>"
            ],
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x290ae13d0>"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = next(iter(val_loader))[0][0].to(device=DEVICE)\n",
        "\n",
        "# list_to_sort = [0] * 10 + [2] * 5 + [1] * 8\n",
        "# Length 23 of [0, 1, 2] repeated\n",
        "# Generating a list of length 23 containing [0, 1, 2] repeated\n",
        "length = 23\n",
        "pattern = [0, 1, 2]\n",
        "\n",
        "# Using list comprehension and modulo to repeat the pattern\n",
        "list_to_sort = [pattern[i % len(pattern)] for i in range(length)]\n",
        "\n",
        "tokens = torch.Tensor([START_TOKEN_ID] + list_to_sort + [MID_TOKEN_ID] + sorted(list_to_sort)).long().to(DEVICE)\n",
        "\n",
        "tokens = tokens[None, :]\n",
        "val_acc = validate(model, tokens)\n",
        "print(val_acc)\n",
        "\n",
        "# Get one input from test_data\n",
        "test_input = tokens[0].detach().cpu()\n",
        "\n",
        "# Pass through model, get cache and predictions\n",
        "logits, cache_model = model.run_with_cache(test_input, remove_batch_dim=True) \n",
        "preds = logits[:, MAX_LIST_LENGTH+1 : -1].argmax(-1)\n",
        "\n",
        "# Get attention pattern and plot it\n",
        "attention_pattern = cache_model[\"pattern\", 0, \"attn\"]\n",
        "tokens_input = list(map(str, test_input))\n",
        "print(test_input)\n",
        "print(preds)\n",
        "\n",
        "cv.attention.attention_patterns(tokens=tokens_input, attention=attention_pattern)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"tensor(4, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(2, device='mps:0')\",\n",
              " \"tensor(2, device='mps:0')\",\n",
              " \"tensor(2, device='mps:0')\",\n",
              " \"tensor(2, device='mps:0')\",\n",
              " \"tensor(2, device='mps:0')\",\n",
              " \"tensor(1, device='mps:0')\",\n",
              " \"tensor(1, device='mps:0')\",\n",
              " \"tensor(1, device='mps:0')\",\n",
              " \"tensor(1, device='mps:0')\",\n",
              " \"tensor(1, device='mps:0')\",\n",
              " \"tensor(1, device='mps:0')\",\n",
              " \"tensor(1, device='mps:0')\",\n",
              " \"tensor(1, device='mps:0')\",\n",
              " \"tensor(5, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(0, device='mps:0')\",\n",
              " \"tensor(1, device='mps:0')\",\n",
              " \"tensor(1, device='mps:0')\",\n",
              " \"tensor(1, device='mps:0')\",\n",
              " \"tensor(1, device='mps:0')\",\n",
              " \"tensor(1, device='mps:0')\",\n",
              " \"tensor(1, device='mps:0')\",\n",
              " \"tensor(1, device='mps:0')\",\n",
              " \"tensor(1, device='mps:0')\",\n",
              " \"tensor(2, device='mps:0')\",\n",
              " \"tensor(2, device='mps:0')\",\n",
              " \"tensor(2, device='mps:0')\",\n",
              " \"tensor(2, device='mps:0')\",\n",
              " \"tensor(2, device='mps:0')\"]"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7o--LiD_HMGs"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
