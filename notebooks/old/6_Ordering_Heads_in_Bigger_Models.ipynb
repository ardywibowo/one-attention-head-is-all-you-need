{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKyC3mZ72qlH"
      },
      "source": [
        "# 1. Sorting Fixed Length Lists with One Head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBe5K-JpHXFs"
      },
      "source": [
        "## Variable hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "httbkgCwHYF4"
      },
      "outputs": [],
      "source": [
        "# Fixed length of list to be sorted\n",
        "LIST_LENGTH = 10\n",
        "\n",
        "# Size of vocabulary\n",
        "D_VOCAB = 66\n",
        "\n",
        "# Should lists have repetitions?\n",
        "ALLOW_REPETITIONS = False\n",
        "\n",
        "# Attention only? (False -> model includes MLPs)\n",
        "ATTN_ONLY = False\n",
        "\n",
        "# Model dimensions\n",
        "N_LAYERS = 1\n",
        "N_HEADS = 1\n",
        "D_MODEL = 128\n",
        "D_HEAD = 32\n",
        "D_MLP = 32\n",
        "\n",
        "if ATTN_ONLY:\n",
        "    D_MLP = None\n",
        "\n",
        "# Default batch size\n",
        "DEFAULT_BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o--LiD_HMGs"
      },
      "source": [
        "## Prelude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yjVSLZbEEJX"
      },
      "source": [
        "### Install and import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wqa5uVY-2oC7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/randyardywibowo/miniconda3/envs/lit/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import transformer_lens\n",
        "except:\n",
        "    !pip install git+https://github.com/neelnanda-io/TransformerLens\n",
        "    !pip install circuitsvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "dqFfW5V32yaO",
        "outputId": "ef6f200d-ea25-4164-acdd-b0ccd98c26c4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div id=\"circuits-vis-5772e133-7878\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, Hello } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-5772e133-7878\",\n",
              "      Hello,\n",
              "      {\"name\": \"You\"}\n",
              "    )\n",
              "    </script>"
            ],
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x1221f87c0>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime as dt\n",
        "from itertools import repeat\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "from typing import cast, Generator, Literal\n",
        "\n",
        "import circuitsvis as cv\n",
        "from fancy_einsum import einsum\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, tensor, Tensor, TensorType as TT\n",
        "from torch.nn import functional as F\n",
        "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
        "from tqdm import tqdm\n",
        "from typing_extensions import Self\n",
        "\n",
        "cv.examples.hello(\"You\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo4oP4dSrYhD"
      },
      "source": [
        "### Invariable hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XwivtyM_Hru",
        "outputId": "956d1292-6d98-4255-c14a-98f84921ed60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE = 'cpu'\n"
          ]
        }
      ],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"{DEVICE = }\")\n",
        "\n",
        "# Seeds to generate training, validation, and test data\n",
        "TRAIN_SEED = 42\n",
        "VAL_SEED = 66\n",
        "TEST_SEED = 1729\n",
        "\n",
        "# Context length: [start, *(unsorted_)list_length, mid, *(sorted_)list_length]\n",
        "N_CTX = 2 * LIST_LENGTH + 2\n",
        "\n",
        "# \"Real\" tokens range from 0 to D_VOCAB - 2 (non-inclusive)\n",
        "VOCAB_MIN_ID = 0\n",
        "VOCAB_MAX_ID = D_VOCAB - 2\n",
        "\n",
        "# START token is D_VOCAB - 2 and MID token is D_VOCAB - 1\n",
        "START_TOKEN_ID = VOCAB_MAX_ID\n",
        "MID_TOKEN_ID = D_VOCAB - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlWGwox63j24"
      },
      "source": [
        "### Data generator and datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "44OZjOq83k1H"
      },
      "outputs": [],
      "source": [
        "def generate_list(batch_size: int) -> Tensor:\n",
        "    if ALLOW_REPETITIONS:\n",
        "        return torch.randint(VOCAB_MIN_ID, VOCAB_MAX_ID, (batch_size, LIST_LENGTH))\n",
        "    return tensor([\n",
        "        random.sample(range(VOCAB_MIN_ID, VOCAB_MAX_ID), k=LIST_LENGTH) \n",
        "        for _ in range(batch_size)\n",
        "    ]).to(DEVICE)\n",
        "\n",
        "# General generator\n",
        "def make_data_gen(\n",
        "    *,\n",
        "    batch_size: int = DEFAULT_BATCH_SIZE,\n",
        "    dataset: Literal[\"train\", \"val\", \"test\"], # probably this arg needs a better name,\n",
        ") -> Generator[Tensor, None, None]:\n",
        "    assert dataset in (\"train\", \"val\", \"test\")\n",
        "    if dataset == \"train\":\n",
        "        seed = TRAIN_SEED\n",
        "    elif dataset == \"val\":\n",
        "        seed = VAL_SEED\n",
        "    else: # test\n",
        "        seed = TEST_SEED\n",
        "    torch.manual_seed(seed)\n",
        "    while True:\n",
        "        # Generate random numbers\n",
        "        x = generate_list(batch_size)\n",
        "        # Sort\n",
        "        x_sorted = torch.sort(x, dim=1).values\n",
        "        # START tokens\n",
        "        x_start = START_TOKEN_ID * torch.ones(batch_size, dtype=torch.int32).reshape(batch_size, -1).to(DEVICE)\n",
        "        # MID tokens\n",
        "        x_mid = MID_TOKEN_ID * torch.ones(batch_size, dtype=torch.int32).reshape(batch_size, -1).to(DEVICE)\n",
        "        yield torch.cat((x_start, x, x_mid, x_sorted), dim=1)\n",
        "\n",
        "\n",
        "# Training data generator (kinda wrapper)\n",
        "def make_train_gen() -> Generator[Tensor, None, None]:\n",
        "    \"\"\"Make generator of training data\"\"\"\n",
        "    return make_data_gen(batch_size=128, dataset=\"train\")\n",
        "\n",
        "# Validation and test data\n",
        "\n",
        "val_data = next(make_data_gen(batch_size=1000, dataset=\"val\"))\n",
        "test_data = next(make_data_gen(batch_size=1000, dataset=\"test\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBJn1fKm3pMX"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PTlNfaKY3qkd"
      },
      "outputs": [],
      "source": [
        "def loss_fn(\n",
        "    logits: Tensor, # [batch, pos, d_vocab] \n",
        "    tokens: Tensor, # [batch, pos] \n",
        "    return_per_token: bool = False\n",
        ") -> Tensor: # scalar\n",
        "    \"\"\"\"\"\"\n",
        "    # \n",
        "    sorted_start_pos = LIST_LENGTH + 2\n",
        "    logits = logits[:, (sorted_start_pos-1):-1]\n",
        "    tokens = tokens[:, sorted_start_pos : None]\n",
        "    log_probs = logits.log_softmax(-1)\n",
        "    correct_log_probs = log_probs.gather(-1, tokens[..., None])[..., 0]\n",
        "    if return_per_token:\n",
        "        return -correct_log_probs\n",
        "    return -correct_log_probs.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKKrzLEq3sun"
      },
      "source": [
        "### Accuracy and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1x6q7b7O3rZa"
      },
      "outputs": [],
      "source": [
        "def get_diff_row_inds(\n",
        "    a: Tensor, # [dim1, dim2]\n",
        "    b: Tensor  # [dim1, dim2]\n",
        ") -> Tensor:   # [dim1]\n",
        "    \"\"\"Find indices of rows where a and b differ\"\"\"\n",
        "    assert a.shape == b.shape\n",
        "    return ((a == b).prod(dim=1) == 0).nonzero(as_tuple=True)[0]\n",
        "\n",
        "def acc_fn(\n",
        "    logits: Tensor, # [batch, pos, d_vocab]\n",
        "    tokens: Tensor, # [batch, pos]\n",
        "    per: Literal[\"token\", \"sequence\"] = \"sequence\"\n",
        ") -> float:\n",
        "    \"\"\"Compute accuracy as percentage of correct predictions\"\"\"\n",
        "    sorted_start_pos = LIST_LENGTH + 2\n",
        "    # Get logits of predictions for position\n",
        "    logits = logits[:, (sorted_start_pos-1):-1]\n",
        "    preds = logits.argmax(-1)\n",
        "    tokens = tokens[:, sorted_start_pos:]\n",
        "    if per == \"sequence\":\n",
        "        return (preds == tokens).prod(dim=1).float().mean().item()\n",
        "    return (preds == tokens).float().mean().item()\n",
        "\n",
        "def validate(\n",
        "    model: HookedTransformer, \n",
        "    data: Tensor # [batch, pos]\n",
        ") -> float:\n",
        "    \"\"\"Test this model on `data`\"\"\"\n",
        "    logits = model(data)\n",
        "    acc = acc_fn(logits, tokens=data)\n",
        "    return acc\n",
        "\n",
        "def show_mispreds(\n",
        "    model: HookedTransformer, \n",
        "    data: Tensor # [batch, pos]\n",
        ") -> None:\n",
        "    \"\"\"Test this model on `data` and print mispredictions\"\"\"\n",
        "    logits = model(data)\n",
        "    sorted_start_pos = LIST_LENGTH + 2\n",
        "    logits = logits[:, (sorted_start_pos-1):-1]\n",
        "    tokens = data[:, sorted_start_pos:]\n",
        "    preds = logits.argmax(-1)\n",
        "    mispred_inds = get_diff_row_inds(tokens, preds)\n",
        "    for i in mispred_inds:\n",
        "        print(f\"[{i}] {tokens[i].numpy().tolist()} | {preds[i].numpy().tolist()}\")\n",
        "    print(f\"{len(mispred_inds)}/{len(preds)} ({len(mispred_inds) / len(preds) :.2%})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5trW5nKhF3I"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Hj7N1nhHrI"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YwIffvImhT4g"
      },
      "outputs": [],
      "source": [
        "cfg = HookedTransformerConfig(\n",
        "    d_model=D_MODEL,\n",
        "    n_layers=N_LAYERS,\n",
        "    n_heads=N_HEADS,\n",
        "    d_head=D_HEAD,\n",
        "    n_ctx=N_CTX,\n",
        "    d_vocab=D_VOCAB,\n",
        "    act_fn=\"relu\",\n",
        "    seed=42,\n",
        "    device=DEVICE,\n",
        "    attn_only=ATTN_ONLY\n",
        ")\n",
        "model = HookedTransformer(cfg, move_to_device=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vYV7QAd3vCT"
      },
      "source": [
        "### Training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LHBPL3Xw3uAt"
      },
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class TrainingHistory:\n",
        "    losses: list[float]\n",
        "    train_accuracies: list[float]\n",
        "    val_accuracies: list[float]\n",
        "\n",
        "def converged(val_accs: list[float], n_last: int = 2) -> bool:\n",
        "    if len(val_accs) < n_last:\n",
        "        return False\n",
        "    return len(set(tensor(val_accs[-n_last:]).round(decimals=4).tolist())) == 1\n",
        "\n",
        "# Number of epochs\n",
        "n_epochs = 20000\n",
        "\n",
        "# Optimization\n",
        "lr = 1e-3\n",
        "betas = (.9, .999)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=lr, betas=betas)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \"min\", patience=100)\n",
        "\n",
        "# Training data generator\n",
        "train_gen = make_train_gen()\n",
        "\n",
        "def train_model(model: HookedTransformer) -> TrainingHistory:\n",
        "    losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        tokens = next(train_gen).to(device=DEVICE)\n",
        "        logits = model(tokens)\n",
        "        loss = loss_fn(logits, tokens)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "        scheduler.step(loss)\n",
        "        \n",
        "        if epoch % 100 == 0:\n",
        "            losses.append(loss.item())\n",
        "            train_batch_acc = acc_fn(logits, tokens)\n",
        "            val_acc = validate(model, val_data)\n",
        "            val_loss = loss_fn(model(val_data), val_data)\n",
        "\n",
        "            train_accuracies.append(train_batch_acc)\n",
        "            val_accuracies.append(val_acc)\n",
        "            print(\n",
        "                f\"Epoch {epoch}/{n_epochs} ({epoch / n_epochs:.0%}) : \"\n",
        "                f\"loss = {loss.item():.4f}; {train_batch_acc=:.3%}; \"\n",
        "                f\"{val_acc=:.3%}; lr={scheduler._last_lr[0]}\" #type:ignore\n",
        "            )\n",
        "            # If last 10 recorded val_accuracies are 100%\n",
        "            if converged(val_accuracies):\n",
        "                print(f\"\\nAchieved consistent perfect validation accuracy after {epoch} epochs\")\n",
        "                break\n",
        "    return TrainingHistory(losses, train_accuracies, val_accuracies)\n",
        "\n",
        "def load_model_state(model: HookedTransformer, filename: str) -> None:\n",
        "    assert os.path.isdir(\"models\"), \"Make a directory `models` with model state dicts\"\n",
        "    if not filename.startswith(\"models/\"):\n",
        "        filename = f\"models/{filename}\"\n",
        "    with open(filename, \"rb\") as f:\n",
        "        state_dict = pickle.load(f)\n",
        "    model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXhR2JBjdXkD"
      },
      "source": [
        "### Train or load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMH8DwNHdWLA",
        "outputId": "8f1cc520-bcda-44a8-c0dd-93b5727d0136"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/20000 (0%) : loss = 4.4698; train_batch_acc=0.000%; val_acc=0.000%; lr=0.001\n",
            "Epoch 100/20000 (0%) : loss = 0.1415; train_batch_acc=75.781%; val_acc=75.100%; lr=0.001\n",
            "Epoch 200/20000 (1%) : loss = 0.0188; train_batch_acc=99.219%; val_acc=97.400%; lr=0.001\n",
            "Epoch 300/20000 (2%) : loss = 0.0131; train_batch_acc=98.438%; val_acc=98.700%; lr=0.001\n",
            "Epoch 400/20000 (2%) : loss = 0.0081; train_batch_acc=98.438%; val_acc=99.400%; lr=0.001\n",
            "Epoch 500/20000 (2%) : loss = 0.0082; train_batch_acc=96.875%; val_acc=99.500%; lr=0.001\n",
            "Epoch 600/20000 (3%) : loss = 0.0023; train_batch_acc=100.000%; val_acc=99.700%; lr=0.001\n",
            "Epoch 700/20000 (4%) : loss = 0.0034; train_batch_acc=100.000%; val_acc=99.700%; lr=0.001\n",
            "\n",
            "Achieved consistent perfect validation accuracy after 700 epochs\n"
          ]
        }
      ],
      "source": [
        "history = train_model(model)\n",
        "# load_model_state(model, <filename>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HABWttcH34dv"
      },
      "source": [
        "### Testing post-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lakshPyuwdO7",
        "outputId": "c7f14878-2871-4194-bbb6-45206c7f6232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validating on validation data:\n",
            "\tval_acc=99.700%\n",
            "\n",
            "[12] [2, 4, 38, 43, 45, 47, 49, 53, 54, 56] | [2, 4, 44, 43, 45, 47, 49, 53, 54, 56]\n",
            "[97] [4, 5, 32, 33, 37, 43, 48, 50, 54, 60] | [4, 5, 33, 33, 37, 43, 48, 50, 54, 60]\n",
            "[275] [6, 8, 9, 13, 14, 16, 23, 59, 61, 63] | [6, 8, 9, 13, 14, 16, 23, 23, 61, 63]\n",
            "3/1000 (0.30%)\n",
            "\n",
            "Validating on test data:\n",
            "\ttest_acc=99.700%\n",
            "\n",
            "[97] [1, 4, 5, 6, 12, 16, 21, 51, 57, 63] | [1, 4, 5, 6, 12, 16, 21, 57, 57, 63]\n",
            "[360] [1, 3, 6, 10, 39, 42, 44, 53, 54, 61] | [1, 3, 6, 10, 42, 42, 44, 53, 54, 61]\n",
            "[711] [0, 12, 13, 47, 51, 53, 54, 55, 57, 62] | [0, 12, 13, 43, 51, 53, 54, 55, 57, 62]\n",
            "3/1000 (0.30%)\n"
          ]
        }
      ],
      "source": [
        "print(\"Validating on validation data:\")\n",
        "val_acc = validate(model, val_data)\n",
        "print(f\"\\t{val_acc=:.3%}\\n\")\n",
        "if val_acc < 1:\n",
        "    show_mispreds(model, val_data)\n",
        "\n",
        "print(\"\\nValidating on test data:\")\n",
        "test_acc = validate(model, test_data)\n",
        "print(f\"\\t{test_acc=:.3%}\\n\")\n",
        "if test_acc < 1:\n",
        "    show_mispreds(model, test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNofXtfSQvh2"
      },
      "source": [
        "### Saving trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "i9lLO9p1d8mC"
      },
      "outputs": [],
      "source": [
        "def save_model_state_dict(\n",
        "    model: HookedTransformer, \n",
        "    filename: str | None = None\n",
        ") -> None:\n",
        "    if not os.path.isdir(\"models\"):\n",
        "        os.mkdir(\"models\")\n",
        "    if not filename:\n",
        "        timestamp = dt.now().isoformat(\"T\", \"minutes\").replace(\":\", \"-\")\n",
        "        filename = f\"model_state_dict_{timestamp}.pkl\"\n",
        "    with open(f\"models/{filename}\", \"wb\") as f:\n",
        "        pickle.dump(model.state_dict(), f)\n",
        "\n",
        "save_model_state_dict(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpp4UQG9ql_u",
        "outputId": "4ee66d44-52ce-4378-f1e1-0454780bea48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['model_state_dict_2023-11-10T13-21.pkl']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir(\"models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnfh-Oxk7sQ9"
      },
      "source": [
        "## Investigate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7po8YMfN4T9"
      },
      "source": [
        "### Attention patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "kPODH527Rc5h",
        "outputId": "a8646e55-b1ca-4743-c8e2-90edade082ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([64, 40, 33, 41, 29, 11, 43, 55,  3, 50, 16, 65,  3, 11, 16, 29, 33, 40,\n",
            "        41, 43, 50, 55])\n",
            "tensor([[ 3, 11, 16, 29, 33, 40, 41, 43, 50, 55]])\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div id=\"circuits-vis-20fb1efa-0130\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-20fb1efa-0130\",\n",
              "      AttentionPatterns,\n",
              "      {\"tokens\": [\"tensor(64)\", \"tensor(40)\", \"tensor(33)\", \"tensor(41)\", \"tensor(29)\", \"tensor(11)\", \"tensor(43)\", \"tensor(55)\", \"tensor(3)\", \"tensor(50)\", \"tensor(16)\", \"tensor(65)\", \"tensor(3)\", \"tensor(11)\", \"tensor(16)\", \"tensor(29)\", \"tensor(33)\", \"tensor(40)\", \"tensor(41)\", \"tensor(43)\", \"tensor(50)\", \"tensor(55)\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0230018962174654, 0.9769980907440186, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.019016345962882042, 0.9461870789527893, 0.0347965769469738, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.021526753902435303, 0.4463498294353485, 0.018923697993159294, 0.5131997466087341, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.029942193999886513, 0.2241954803466797, 0.5918201804161072, 0.13488996028900146, 0.019152220338582993, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3117559850215912, 0.12073250859975815, 0.2182769626379013, 0.13714943826198578, 0.11535703390836716, 0.09672801196575165, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0684022381901741, 0.06644414365291595, 0.03728918731212616, 0.3546485900878906, 0.006706493441015482, 0.08357705920934677, 0.3829323649406433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09389834105968475, 0.0139335161074996, 0.005394938867539167, 0.012102378532290459, 0.0026752054691314697, 0.22930452227592468, 0.027877332642674446, 0.6148136854171753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.021940553560853004, 0.01715930365025997, 0.0739675909280777, 0.010600125417113304, 0.0516250804066658, 0.6008903384208679, 0.000790208694525063, 0.00976896844804287, 0.2132577747106552, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.011452998034656048, 0.007636617869138718, 0.001513018854893744, 0.005012707784771919, 0.0013414905406534672, 0.031152229756116867, 0.004132355563342571, 0.5931029319763184, 0.24336937069892883, 0.10128631442785263, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06600923091173172, 0.07364067435264587, 0.19501613080501556, 0.12763822078704834, 0.31890010833740234, 0.013797721825540066, 0.04024432599544525, 0.0031443750485777855, 0.01546013355255127, 0.035856593400239944, 0.11029249429702759, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00011142873700009659, 7.397602894343436e-05, 0.00014548619219567627, 9.933509863913059e-05, 0.0004330104566179216, 0.0204213485121727, 0.00012159675679868087, 1.7282542103203014e-05, 0.9726026058197021, 8.618074389232788e-06, 0.005765590351074934, 0.00019976937619503587, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01544894278049469, 0.004138908814638853, 0.029327603057026863, 0.0027698262128978968, 0.05109034478664398, 0.5554181933403015, 0.0003096990112680942, 0.0010031320853158832, 0.03647342696785927, 0.001254063448868692, 0.2664007246494293, 0.03135089948773384, 0.00501421419903636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02252543717622757, 0.02057597041130066, 0.043792061507701874, 0.02522900141775608, 0.08431071788072586, 0.02740064635872841, 0.006102571729570627, 0.0014827913837507367, 0.0005990033387206495, 0.0016714048106223345, 0.7644907832145691, 0.00084116798825562, 2.3733915440971032e-05, 0.0009548186208121479, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.033847298473119736, 0.06806565076112747, 0.20961284637451172, 0.0861646756529808, 0.5006194710731506, 0.012287260964512825, 0.04190531745553017, 0.0007794577977620065, 0.0007350300438702106, 0.015592915005981922, 0.027690112590789795, 0.0003304124402347952, 0.00010295227548340335, 0.0008372419979423285, 0.0014293037820607424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.012442689388990402, 0.19122590124607086, 0.4709003269672394, 0.11175280809402466, 0.03543749079108238, 0.01133923139423132, 0.12262874841690063, 0.00797695480287075, 0.0001547169522382319, 0.02986155077815056, 0.0012315064668655396, 0.0007244038861244917, 2.575803955551237e-05, 0.002616318641230464, 3.6661371268564835e-05, 0.0016449246322736144, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.001844519516453147, 0.30793577432632446, 0.02489595115184784, 0.2605591416358948, 0.008717968128621578, 0.0017443086253479123, 0.3047126829624176, 0.029856789857149124, 0.00017827060946729034, 0.05855876952409744, 0.00016371236415579915, 0.00027189101092517376, 1.8363271010457538e-05, 0.00015367800369858742, 6.487366590590682e-06, 0.00019879560568369925, 0.00018286073463968933, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00043227747664786875, 0.04336882755160332, 0.003361822571605444, 0.37482085824012756, 0.0009119653841480613, 0.0024790179450064898, 0.36227044463157654, 0.0792032778263092, 0.003464150009676814, 0.1265551894903183, 0.00032770444522611797, 0.00033282028743997216, 0.0004893653094768524, 0.0003485005581751466, 4.399334284244105e-05, 5.212126052356325e-05, 6.541793118230999e-05, 0.0014722517225891352, 0.0, 0.0, 0.0, 0.0], [0.0002534742816351354, 0.0173528790473938, 0.0007113395840860903, 0.017653344199061394, 0.001144982292316854, 0.0036548192147165537, 0.7895791530609131, 0.0618126280605793, 0.0052522923797369, 0.09825894981622696, 0.00017256867431569844, 0.0001620622497284785, 0.0012059753062203526, 0.0010604625567793846, 2.6585048544802703e-05, 0.00014495626965072006, 1.8539862139732577e-05, 0.0003519365272950381, 0.0011831432348117232, 0.0, 0.0, 0.0], [0.001012285822071135, 0.010718914680182934, 0.002390228444710374, 0.033040329813957214, 0.000615015858784318, 0.0026264272164553404, 0.07956016808748245, 0.2813974916934967, 0.000983365811407566, 0.5833227634429932, 0.0015969125088304281, 0.0002849510929081589, 0.0001900077040772885, 0.00037385933683253825, 0.00011131077189929783, 5.4414707847172394e-05, 3.102308255620301e-05, 0.00017598448903299868, 0.0003117878222838044, 0.0012028227793052793, 0.0, 0.0], [0.0010549810249358416, 0.0032564338762313128, 0.0004862149362452328, 0.001107291434891522, 0.0001887226098915562, 0.001826324500143528, 0.003953334875404835, 0.8727481961250305, 0.0004065855755470693, 0.10398758202791214, 0.0018121820176020265, 0.0039235069416463375, 0.00040975079173222184, 0.0009215974132530391, 0.0018365737050771713, 7.48840466258116e-05, 8.587201591581106e-05, 7.297447154996917e-05, 4.3877917050849646e-05, 4.7061836085049435e-05, 0.0017560309497639537, 0.0], [0.016704916954040527, 0.026068154722452164, 0.004704558756202459, 0.011485125869512558, 0.003601141506806016, 0.03191886469721794, 0.06823564320802689, 0.2539145350456238, 0.09848813712596893, 0.16650961339473724, 0.026622900739312172, 0.012468444183468819, 0.09913480281829834, 0.032380737364292145, 0.04523661732673645, 0.0056754061952233315, 0.0036395585630089045, 0.007367715239524841, 0.0032082744874060154, 0.022651609033346176, 0.00971447117626667, 0.050268735736608505]]]}\n",
              "    )\n",
              "    </script>"
            ],
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x29a2eae20>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get one input from test_data\n",
        "test_input = test_data[3, :]\n",
        "\n",
        "# Pass through model, get cache and predictions\n",
        "logits, cache_model = model.run_with_cache(test_input, remove_batch_dim=True) \n",
        "preds = logits[:, LIST_LENGTH+1 : -1].argmax(-1)\n",
        "\n",
        "# Get attention pattern and plot it\n",
        "attention_pattern = cache_model[\"pattern\", 0, \"attn\"]\n",
        "tokens_input = list(map(str, test_input))\n",
        "print(test_input)\n",
        "print(preds)\n",
        "\n",
        "cv.attention.attention_patterns(tokens=tokens_input, attention=attention_pattern)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWbRRSo8Xnxz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PBe5K-JpHXFs"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
